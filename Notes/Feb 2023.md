ToDo List

1. AIGS sugarcane revision 
2. Multi head block attention network
3. posterior distribution detection
4. shap investigation 



Bayesian investigate

1. posterior inclusion probability (summary Bayesian)
2. 200511_SBayes_Jackie_Kiewa.pptx (details)

Machine learning parental selection
Khaki S, Khalilzadeh Z, Wang L (2020) Predicting yield performance of parents in plant breeding: A neural collaborative filtering approach. PLOS ONE 15(5): e0233382. https://doi.org/10.1371/journal.pone.0233382



Song, W., Shi, C., Xiao, Z., Duan, Z., Xu, Y., Zhang, M., & Tang, J. (2019, November). Autoint: Automatic feature interaction learning via self-attentive neural networks. In *Proceedings of the 28th ACM International Conference on Information and Knowledge Management* (pp. 1161-1170).

![AutoInt:使用Multi-head Self-Attention进行自动特征学习的CTR模型](https://picx.zhimg.com/v2-4875432d62160ab97a87a17d900dd704_720w.jpg?source=172ae18b)

```bash
composer="/afm03/Q4/Q4179/ML_composer/"
ls -lh $composer
cp -r $composer $TMPDIR/
locat=`pwd`
geno="data/sugarcane_disease"
pheno="data/sugarcane_disease.phen"
index="data/sugarcane_disease.index"
anno="data/Annotation/SNPinfo-0.4-4.txt"
target="./Attention_Test"
model="MultiHead Attention LNN"
width=256
depth=0

python $TMPDIR/ML_composer/GS_composer.py --ped $geno --pheno $pheno --mpheno 1 --index $index --annotation $anno --trait smut --width $width --depth $depth --model "MultiHead Attention LNN" -o $target --quiet 0 --plot --epoch 10 --num-heads 2 --batch 2 --lr 0.0001

python $TMPDIR/ML_composer/GS_composer.py --ped $geno --pheno $pheno --mpheno 1 --index $index --trait smut --width $width --depth $depth --model "Attention CNN" -o $target --quiet 1 --plot --epoch 15 --num-heads 2 --lr 0.001 --residual --batch 8

#On submit script
python $TMPDIR/ML_composer/GS_composer.py --ped $geno --pheno $pheno --mpheno 1 --index $index --trait smut --width $width --depth $depth --model "Attention CNN" -o $target --quiet 0 --plot --epoch 80 --num-heads 2 --lr 0.01 --residual
python $TMPDIR/ML_composer/GS_composer.py --ped $geno --pheno $pheno --mpheno 2 --index $index --trait pachy --width $width --depth $depth --model "Attention CNN" -o $target --quiet 0 --plot --epoch 80 --num-heads 2 --lr 0.01 --residual
```



Self designed SNP global attention:

Raw Q,K,V shape: (batch, seq, 1,embed) or (batch, seq, embed)

Wk = (seq,1,embed) or (seq,embed) = 2608xembed 8 

Wv = (seq,embed) x (embed,second_embed) = (seq,second_embed) = 2606xembed + embedxsecondEmbed 8


$$
Attention\ Score (A) = softmax(Q(W_kK)^T)
$$
Output shape: (batch,Seq,Seq,embed) or (batch, seq, seq) 

Optional (Attention convolution) by (embeding//heads,1), filter: second embedding, step:1 -- get shape (batch, seq, seq,second_embeding) = embedding x second Embed= 64

Kernel shape divided by heads?


$$
Effect\ Score (E) = A(W_V \cdot V)
$$
output shape: (batch,seq,seq,second_embed)

Second convolution: kernel (seq, second_mebed), filter, get output shape: (batch, seq, filter) 2608 x 8

1xfilter convolution,1 filter

Global average pooling

get prediction



LD x SNP attention

LD~LD Attention (LDxLD) Individual SNP were grouped as LD 0, others by LD number

​	shape (LD,LD)

Inside-LD Attention using Mask

Estimate GEBV:
$$
GEBV = M_{SNP} * (W_{LD}*W_{LD-SNP}) 
$$
M_SNP = (B, S, LD) (LD from 1~n+1) individual SNP grouped as LD 0

M_LD = (LD,1) (LD from 0~n)

M_LD_SNP = (S, LD) 

Final SNP weight = SNP alleles * LD weight (with attention) * SNP weight~LD (with LD)



Attention drop out layer

 Choe, J., & Shim, H. (2019). Attention-based dropout layer for weakly supervised object localization. In *Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition* (pp. 2219-2228).



Currently running DL tasks:

LNN_SNPMAP_Attention_single 

LNN_MultiHead_Attention (Standard)

LNN_ConvEmbedding_MultiHead_Attention

LNN_SNPMAP_ConvEmbedding_MultiHead_Attention

LD_Attention



```R
library(dplyr)
library(tidyr)
library(stringr)
rs = c(0.1,0.2,0.5,0.7,0.9,1,2,3,4,5)
tors = c(1,2,3,4,5)
for (r in rs){
	actual_r = r/10
	for (tor in tors) {
        filename = paste0("LDs/LD-SNPs-",r,"-",tor,".txt")
        print(filename)
        lds = read.table(filename,sep=" ",h=T)
        lds$LID = rownames(lds)
		lds$Scount = str_count(lds$Markers, ";")
        summary(lds$Scount)
        if (1 %in% lds$Scount){
        	lds[which(lds$Scount == 1),]$LID = 0}
        lds$Markers = as.character(lds$Markers)
        lds %>% 
		mutate(SID = strsplit(Markers,";")) %>% 
		unnest(SID) %>% select(c("SID","LID")) %>% write.table(paste0("SNP_LD_0/SNPinfo-",actual_r,"-",tor,".txt"),row.names=F,quote=F,col.names=T,sep="\t")
    }
}


```

https://openai.com/research/sparse-transformer



Test accuracy dependency for attention model on embedding/blocking channels



New script

```bash
composer="/afm03/Q4/Q4179/ML_composer/"
ls -lh $composer
cp -r $composer $TMPDIR/
locat=`pwd`
geno="data/sugarcane_disease"
pheno="data/sugarcane_disease.phen"
index="data/sugarcane_disease.index"
anno="data/Annotation/SNPinfo-0.4-4.txt"
target="./Attention_Test"
model="MultiHead Attention LNN"
width=256
depth=2

python $TMPDIR/ML_composer/GS_composer.py --ped $geno --pheno $pheno --mpheno 1 --index $index --annotation $anno --trait smut --width $width --depth $depth --model "MultiHead Attention LNN" -o $target --quiet 1 --round 1 --plot --epoch 10 --embedding 64 --num-heads 1 --batch 12 --lr 0.0001 --residual


######using subset########
composer="/afm03/Q4/Q4179/ML_composer/"
ls -lh $composer
cp -r $composer $TMPDIR/
locat=`pwd`
geno="data/subset/disease_subset"
pheno="data/subset/disease_subset.phen"
index="data/subset/disease_subset.index"
anno="data/subset/disease_subset.anno"
target="./Attention_Test"
model="MultiHead Attention LNN"
width=256
depth=2

python $TMPDIR/ML_composer/GS_composer.py --ped $geno --pheno $pheno --mpheno 1 --index $index --annotation $anno --trait smut --width $width --depth $depth --model "MultiHead Attention LNN" -o $target --quiet 1 --round 1 --plot --epoch 10 --embedding 64 --num-heads 1 --batch 64 --lr 0.0001 --residual
```

