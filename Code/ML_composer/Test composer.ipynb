{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "aeec0071",
   "metadata": {},
   "source": [
    "TEST ML_composer \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a07c74aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "E:\\learning resource\\PhD\\PHD_Notebook\\Code\\ML_composer\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "print(os.getcwd())\n",
    "#os.chdir(\"F:/project/sugarcane_disease/\")\n",
    "#os.chdir(\"O:/project/sugarcane_disease/\")\n",
    "os.getcwd()\n",
    "os.environ[\"KMP_DUPLICATE_LIB_OK\"]=\"TRUE\"\n",
    "os.chdir(\"E:/learning resource/OneDrive - The University of Queensland/PhD/HPC_Results/Sugarcane_disease/ML/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4466a1cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From E:\\learning resource\\PhD\\PHD_Notebook\\Code\\ML_composer\\ClassModel.py:31: experimental_run_functions_eagerly (from tensorflow.python.eager.def_function) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.config.run_functions_eagerly` instead of the experimental version.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from GS_composer import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3dc4db23",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ClassModel import *\n",
    "from CustomLayers import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ab67e0f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from Functions import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "06d3107f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class p_args:\n",
    "    \n",
    "    def __init__(self):\n",
    "        \n",
    "        self.ped = \"E:/learning resource/OneDrive - The University of Queensland/PhD/data/sugarcane_disease/subset/disease_subset\"\n",
    "        self.pheno = \"E:/learning resource/OneDrive - The University of Queensland/PhD/data/sugarcane_disease/subset/disease_subset.phen\"\n",
    "        self.mpheno = 1\n",
    "        self.index = \"E:/learning resource/OneDrive - The University of Queensland/PhD/data/sugarcane_disease/subset/disease_subset.index\"\n",
    "        self.model = \"MultiHead Attention LNN\"\n",
    "        self.annotation = None# \"E:/learning resource/OneDrive - The University of Queensland/PhD/data/sugarcane_disease/subset/disease_subset.anno\"\n",
    "        self.trait = \"smut\"\n",
    "        self.output = \"./Attention_Test_win\"\n",
    "        \n",
    "        self.load = None\n",
    "        self.save = True\n",
    "        self.plot = True\n",
    "        \n",
    "        self.round = 1\n",
    "        self.epoch = 10\n",
    "        self.rank =False\n",
    "        self.lr = 0.0001\n",
    "        self.batch = 32\n",
    "        self.mean = False\n",
    "        self.width = 64\n",
    "        self.depth = 2\n",
    "        self.quiet = 1\n",
    "        self.num_heads = 1\n",
    "        self.residual = True\n",
    "        self.embedding = 12\n",
    "        \n",
    "        \n",
    "args = p_args()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2ef28731",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d53b3f4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "if args.output[0] == \"/\":\n",
    "    locat = '/' + args.output.strip('/') + '/'\n",
    "else:\n",
    "    locat = args.output.strip('/') + '/'\n",
    "if not os.path.exists(locat):\n",
    "    os.mkdir(locat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4c3ae127",
   "metadata": {},
   "outputs": [],
   "source": [
    "composer = ML_composer()\n",
    "composer._model\n",
    "composer.silence_mode = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "933a9a84",
   "metadata": {},
   "outputs": [],
   "source": [
    "#composer.get_data(config,args)\n",
    "composer.args = args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fbf7911d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1    939\n",
      "3    940\n",
      "2    940\n",
      "4    940\n",
      "5    943\n",
      "Name: 2, dtype: int64\n",
      "Get genotype shape: (4702, 100)\n",
      "   7   8   9   10  11  12  13  14  15\n",
      "1   0   2   0   1   2   1   0   1   2\n",
      "2   1   2   1   1   2   1   0   1   1\n",
      "3   1   2   0   0   1   0   1   1   2\n",
      "4   1   2   0   0   1   0   1   1   2\n",
      "5   0   2   1   1   2   0   2   1   1\n",
      "6   0   2   0   1   2   1   0   1   2\n",
      "7   1   2   1   1   2   1   2   1   2\n",
      "8   0   2   0   1   1   0   1   0   2\n",
      "9   1   2   1   1   2   1   2   1   2\n",
      "Running data check\n",
      "GENO\n",
      "FAM\n",
      "PHENO\n",
      "INDEX\n"
     ]
    }
   ],
   "source": [
    "composer.get_data(config,args=args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20b38e5b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "303be834",
   "metadata": {},
   "outputs": [],
   "source": [
    "composer.prepare_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "035137c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "index_ref = composer.prepare_cross_validate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c6ce7840",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross-validate: ([2, 3, 4, 5], [1])\n"
     ]
    }
   ],
   "source": [
    "print(\"Cross-validate: {}\".format(index_ref[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1a7c8651",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_idx,valid_idx = index_ref[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "48131b6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall population: 4702\n",
      "759 individuals need to be removed due to the miss phenotype\n",
      "Filtered population: 3943\n",
      "Mean of train phenotype: 4.715129635707253\n",
      "Use raw phenotype as the target\n",
      "16    1.0\n",
      "39    3.0\n",
      "44    3.0\n",
      "47    2.0\n",
      "92    3.0\n",
      "Name: 2, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "composer.prepare_training(train_idx,valid_idx)\n",
    "#composer.batchSize = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d63bbccb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#composer._raw_data[\"GENO\"].iloc[:,6:].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "737df705",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "USE Attention CNN MODEL as training method\n",
      "All the SNPs are already decoded, imputing missing SNPs with 0.01\n",
      "Convert data to np.array float32\n",
      "The transformed SNP shape: (3047, 100, 1)\n",
      "USE Attention CNN MODEL as training method\n",
      "All the SNPs are already decoded, imputing missing SNPs with 0.01\n",
      "Convert data to np.array float32\n",
      "The transformed SNP shape: (896, 100, 1)\n",
      "Train status:\n",
      "Epochs:  10\n",
      "Repeat(Round):  1\n",
      "Model: \"functional_1\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_layer_1 (InputLayer)      [(None, 100, 1)]     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "zero_padding1d (ZeroPadding1D)  (None, 100, 1)       0           input_layer_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "locally_connected1d (LocallyCon (None, 10, 12)       1200        zero_padding1d[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, 10, 12)       156         locally_connected1d[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "multi_head_qkv__block_attention ((None, 10, 12), (No 432         dense[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "add (Add)                       (None, 10, 12)       0           multi_head_qkv__block_attention[0\n",
      "                                                                 dense[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization (LayerNorma (None, 10, 12)       24          add[0][0]                        \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 10, 12)       156         layer_normalization[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "add_1 (Add)                     (None, 10, 12)       0           dense_1[0][0]                    \n",
      "                                                                 layer_normalization[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "re_lu (ReLU)                    multiple             0           add_1[0][0]                      \n",
      "                                                                 add_3[0][0]                      \n",
      "                                                                 add_4[0][0]                      \n",
      "                                                                 dense_5[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "multi_head_qkv__block_attention ((None, 10, 12), (No 432         re_lu[0][0]                      \n",
      "                                                                 multi_head_qkv__block_attention[0\n",
      "__________________________________________________________________________________________________\n",
      "add_2 (Add)                     (None, 10, 12)       0           multi_head_qkv__block_attention_1\n",
      "                                                                 re_lu[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_1 (LayerNor (None, 10, 12)       24          add_2[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 10, 12)       156         layer_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "add_3 (Add)                     (None, 10, 12)       0           dense_2[0][0]                    \n",
      "                                                                 layer_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "flatten (Flatten)               (None, 120)          0           re_lu[1][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "dropout (Dropout)               (None, 120)          0           flatten[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, 64)           7744        dropout[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_4 (Dense)                 (None, 64)           4160        dense_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "add_4 (Add)                     (None, 64)           0           dense_3[0][0]                    \n",
      "                                                                 dense_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_5 (Dense)                 (None, 64)           4160        re_lu[2][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "dense_6 (Dense)                 (None, 1)            65          re_lu[3][0]                      \n",
      "==================================================================================================\n",
      "Total params: 18,709\n",
      "Trainable params: 18,709\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "None\n",
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\pc\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\python\\data\\ops\\dataset_ops.py:3350: UserWarning: Even though the tf.config.experimental_run_functions_eagerly option is set, this option does not apply to tf.data functions. tf.data functions are still traced and executed as graphs.\n",
      "  \"Even though the tf.config.experimental_run_functions_eagerly \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "96/96 [==============================] - 5s 53ms/step - loss: 6.9340 - val_loss: 4.3939\n",
      "Epoch 2/10\n",
      "96/96 [==============================] - 4s 43ms/step - loss: 5.7152 - val_loss: 4.1812\n",
      "Epoch 3/10\n",
      "96/96 [==============================] - 4s 38ms/step - loss: 5.5326 - val_loss: 4.0964\n",
      "Epoch 4/10\n",
      "96/96 [==============================] - 4s 39ms/step - loss: 5.5110 - val_loss: 3.8506\n",
      "Epoch 5/10\n",
      "96/96 [==============================] - 4s 38ms/step - loss: 5.4207 - val_loss: 3.9103\n",
      "Epoch 6/10\n",
      "96/96 [==============================] - 4s 39ms/step - loss: 5.3296 - val_loss: 3.9245\n",
      "Epoch 7/10\n",
      "96/96 [==============================] - 4s 39ms/step - loss: 5.3261 - val_loss: 4.0687\n",
      "Epoch 8/10\n",
      "96/96 [==============================] - 4s 38ms/step - loss: 5.3575 - val_loss: 3.8305\n",
      "Epoch 9/10\n",
      "96/96 [==============================] - 4s 39ms/step - loss: 5.1878 - val_loss: 3.8828\n",
      "Epoch 10/10\n",
      "96/96 [==============================] - 4s 38ms/step - loss: 5.2291 - val_loss: 3.8867\n",
      " - train loss     : 5.229065895080566\n",
      " - validation loss: 3.8866522312164307\n",
      " - loss decrease rate in last 5 epochs: 0.007221579551696777\n",
      " - Actual Training epochs:  10\n",
      "Train End.\n",
      "In-year accuracy (measured as Pearson's correlation) is:  0.33545160098877375\n",
      "Training Runtime:  0.7166666666666667  min\n",
      "Checking memory usage is not currently available.\n",
      "USE Attention CNN MODEL as training method\n",
      "All the SNPs are already decoded, imputing missing SNPs with 0.01\n",
      "Convert data to np.array float32\n",
      "The transformed SNP shape: (896, 100, 1)\n",
      "Predicting valid set..\n",
      "Testing prediction:\n",
      "Predicted:  [4.031829  3.521931  4.139615  3.8530748 4.4515657 4.6846895 4.4471455\n",
      " 3.489934  3.4052048 3.942727 ]\n",
      "observed:  [1. 3. 3. 2. 3. 4. 2. 3. 3. 4.]\n",
      "Validate prediction accuracy (measured as Pearson's correlation) is:  0.12469911956135869\n",
      "Saving the model with higher accuracy...\n",
      "WARNING:tensorflow:From C:\\Users\\pc\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\python\\training\\tracking\\tracking.py:111: Model.state_updates (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This property should not be used in TensorFlow 2.0, as updates are applied automatically.\n",
      "WARNING:tensorflow:From C:\\Users\\pc\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\python\\training\\tracking\\tracking.py:111: Layer.updates (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This property should not be used in TensorFlow 2.0, as updates are applied automatically.\n",
      "Saving model failed, tring directly save by using self._model[\"TRAINED_MODEL\"].save\n",
      "Resource check:\n",
      "Total memory: 31.9502 GB\n",
      "Currently using memory: 2.2609 GB\n",
      "Ratio of used memory: 70.3000  %\n",
      "Number of CPU node:  16\n",
      "Plotting the training process...\n",
      "E:\\learning resource\\OneDrive - The University of Queensland\\PhD\\HPC_Results\\Sugarcane_disease\\ML\\Attention_Test_win/smut_MultiHeadAttentionLNN_smut\n",
      "Plotting loss history...\n",
      "Plot name:  E:\\learning resource\\OneDrive - The University of Queensland\\PhD\\HPC_Results\\Sugarcane_disease\\ML\\Attention_Test_win/smut_MultiHeadAttentionLNN_smut/smut_MultiHeadAttentionLNN_1.png\n",
      "Result:\n",
      "  Trait      TrainSet ValidSet                  Model  Test_Accuracy  \\\n",
      "0  smut  [2, 3, 4, 5]      [1]  MultiHeadAttentionLNN       0.335452   \n",
      "\n",
      "   Valid_Accuracy       MSE   Runtime  \n",
      "0        0.124699  3.886652  0.716667  \n"
     ]
    }
   ],
   "source": [
    "composer.compose(train_idx,valid_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b6bf9029",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./test_model\\assets\n",
      "Default GPU Device:/device:GPU:0\n"
     ]
    }
   ],
   "source": [
    "composer._model[\"TRAINED_MODEL\"].save(\"./test_model\")\n",
    "\n",
    "#import tensorflow as tf \n",
    "\n",
    "if tf.test.gpu_device_name(): \n",
    "    print('Default GPU Device:{}'.format(tf.test.gpu_device_name()))\n",
    "else:\n",
    "    print(\"Please install GPU version of TF\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "98683467",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1.5//1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4ef0dd7a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'E:\\\\learning resource\\\\OneDrive - The University of Queensland\\\\PhD\\\\HPC_Results\\\\Sugarcane_disease\\\\ML'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b0d8d8d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0c45b7c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe44c173",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2be5864f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross-validate: 1\n",
      "Overall population: 4702\n",
      "759 individuals need to be removed due to the miss phenotype\n",
      "Filtered population: 3943\n",
      "Mean of train phenotype: 4.715129635707253\n",
      "Use raw phenotype as the target\n",
      "16    1.0\n",
      "39    3.0\n",
      "44    3.0\n",
      "47    2.0\n",
      "92    3.0\n",
      "Name: 2, dtype: float64\n",
      "USE Attention CNN MODEL as training method\n",
      "All the SNPs are already decoded, imputing missing SNPs with 0.01\n",
      "Convert data to np.array float32\n",
      "The transformed SNP shape: (3047, 100, 1)\n",
      "USE Attention CNN MODEL as training method\n",
      "All the SNPs are already decoded, imputing missing SNPs with 0.01\n",
      "Convert data to np.array float32\n",
      "The transformed SNP shape: (896, 100, 1)\n",
      "Train status:\n",
      "Epochs:  10\n",
      "Repeat(Round):  1\n",
      "Model: \"functional_1\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_layer_1 (InputLayer)      [(None, 100, 1)]     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "zero_padding1d (ZeroPadding1D)  (None, 100, 1)       0           input_layer_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "locally_connected1d (LocallyCon (None, 10, 12)       1200        zero_padding1d[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, 10, 12)       156         locally_connected1d[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "multi_head_qkv__block_attention ((None, 10, 12), (No 432         dense[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "add (Add)                       (None, 10, 12)       0           multi_head_qkv__block_attention[0\n",
      "                                                                 dense[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization (LayerNorma (None, 10, 12)       24          add[0][0]                        \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 10, 12)       156         layer_normalization[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "add_1 (Add)                     (None, 10, 12)       0           dense_1[0][0]                    \n",
      "                                                                 layer_normalization[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "re_lu (ReLU)                    multiple             0           add_1[0][0]                      \n",
      "                                                                 add_3[0][0]                      \n",
      "                                                                 add_4[0][0]                      \n",
      "                                                                 dense_5[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "multi_head_qkv__block_attention ((None, 10, 12), (No 432         re_lu[4][0]                      \n",
      "                                                                 multi_head_qkv__block_attention[0\n",
      "__________________________________________________________________________________________________\n",
      "add_2 (Add)                     (None, 10, 12)       0           multi_head_qkv__block_attention_1\n",
      "                                                                 re_lu[4][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_1 (LayerNor (None, 10, 12)       24          add_2[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 10, 12)       156         layer_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "add_3 (Add)                     (None, 10, 12)       0           dense_2[0][0]                    \n",
      "                                                                 layer_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "flatten (Flatten)               (None, 120)          0           re_lu[5][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "dropout (Dropout)               (None, 120)          0           flatten[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, 64)           7744        dropout[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_4 (Dense)                 (None, 64)           4160        dense_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "add_4 (Add)                     (None, 64)           0           dense_3[0][0]                    \n",
      "                                                                 dense_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_5 (Dense)                 (None, 64)           4160        re_lu[6][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "dense_6 (Dense)                 (None, 1)            65          re_lu[7][0]                      \n",
      "==================================================================================================\n",
      "Total params: 18,709\n",
      "Trainable params: 18,709\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "None\n",
      "Epoch 1/10\n",
      " 3/96 [..............................] - ETA: 2s - loss: 7.9373"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\pc\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\python\\data\\ops\\dataset_ops.py:3350: UserWarning: Even though the tf.config.experimental_run_functions_eagerly option is set, this option does not apply to tf.data functions. tf.data functions are still traced and executed as graphs.\n",
      "  \"Even though the tf.config.experimental_run_functions_eagerly \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "96/96 [==============================] - 4s 46ms/step - loss: 6.4289 - val_loss: 3.9535\n",
      "Epoch 2/10\n",
      "96/96 [==============================] - 4s 42ms/step - loss: 5.7313 - val_loss: 3.9219\n",
      "Epoch 3/10\n",
      "96/96 [==============================] - 4s 39ms/step - loss: 5.6665 - val_loss: 3.9190\n",
      "Epoch 4/10\n",
      "96/96 [==============================] - 4s 37ms/step - loss: 5.5015 - val_loss: 3.9187\n",
      "Epoch 5/10\n",
      "96/96 [==============================] - 4s 38ms/step - loss: 5.4170 - val_loss: 3.8840\n",
      "Epoch 6/10\n",
      "96/96 [==============================] - 4s 40ms/step - loss: 5.2962 - val_loss: 4.0400\n",
      "Epoch 7/10\n",
      "96/96 [==============================] - 4s 39ms/step - loss: 5.3590 - val_loss: 4.0184\n",
      "Epoch 8/10\n",
      "96/96 [==============================] - 4s 38ms/step - loss: 5.3032 - val_loss: 3.9050\n",
      "Epoch 9/10\n",
      "96/96 [==============================] - 4s 39ms/step - loss: 5.1882 - val_loss: 3.9082\n",
      "Epoch 10/10\n",
      "96/96 [==============================] - 4s 38ms/step - loss: 5.2086 - val_loss: 3.9221\n",
      " - train loss     : 5.208585739135742\n",
      " - validation loss: 3.9220752716064453\n",
      " - loss decrease rate in last 5 epochs: -0.02437264919281006\n",
      " - Actual Training epochs:  10\n",
      "Train End.\n",
      "In-year accuracy (measured as Pearson's correlation) is:  0.3151745135351908\n",
      "Training Runtime:  0.65  min\n",
      "Checking memory usage is not currently available.\n",
      "USE Attention CNN MODEL as training method\n",
      "All the SNPs are already decoded, imputing missing SNPs with 0.01\n",
      "Convert data to np.array float32\n",
      "The transformed SNP shape: (896, 100, 1)\n",
      "Predicting valid set..\n",
      "Testing prediction:\n",
      "Predicted:  [3.5584664 3.9072454 3.7790298 3.2338417 4.1590796 3.4851735 3.4207187\n",
      " 4.1697817 3.8138707 3.0248938]\n",
      "observed:  [1. 3. 3. 2. 3. 4. 2. 3. 3. 4.]\n",
      "Validate prediction accuracy (measured as Pearson's correlation) is:  0.07336242920785796\n",
      "Saving the model with higher accuracy...\n",
      "Saving model failed, tring directly save by using self._model[\"TRAINED_MODEL\"].save\n",
      "Resource check:\n",
      "Total memory: 31.9502 GB\n",
      "Currently using memory: 2.2741 GB\n",
      "Ratio of used memory: 71.4000  %\n",
      "Number of CPU node:  16\n",
      "Plotting the training process...\n",
      "E:\\learning resource\\OneDrive - The University of Queensland\\PhD\\HPC_Results\\Sugarcane_disease\\ML\\Attention_Test_win/smut_MultiHeadAttentionLNN_smut\n",
      "Plotting loss history...\n",
      "Plot name:  E:\\learning resource\\OneDrive - The University of Queensland\\PhD\\HPC_Results\\Sugarcane_disease\\ML\\Attention_Test_win/smut_MultiHeadAttentionLNN_smut/smut_MultiHeadAttentionLNN_1.png\n",
      "Result:\n",
      "  Trait      TrainSet ValidSet                  Model  Test_Accuracy  \\\n",
      "0  smut  [2, 3, 4, 5]      [1]  MultiHeadAttentionLNN       0.335452   \n",
      "1  smut  [2, 3, 4, 5]      [1]  MultiHeadAttentionLNN       0.315175   \n",
      "\n",
      "   Valid_Accuracy       MSE   Runtime  \n",
      "0        0.124699  3.886652  0.716667  \n",
      "1        0.073362  3.922075  0.650000  \n",
      "Cross-validate: 2\n",
      "Overall population: 4702\n",
      "759 individuals need to be removed due to the miss phenotype\n",
      "Filtered population: 3943\n",
      "Mean of train phenotype: 4.604046242774566\n",
      "Use raw phenotype as the target\n",
      "18    5.0\n",
      "28    1.0\n",
      "29    2.0\n",
      "30    6.0\n",
      "33    1.0\n",
      "Name: 2, dtype: float64\n",
      "USE Attention CNN MODEL as training method\n",
      "All the SNPs are already decoded, imputing missing SNPs with 0.01\n",
      "Convert data to np.array float32\n",
      "The transformed SNP shape: (3114, 100, 1)\n",
      "USE Attention CNN MODEL as training method\n",
      "All the SNPs are already decoded, imputing missing SNPs with 0.01\n",
      "Convert data to np.array float32\n",
      "The transformed SNP shape: (829, 100, 1)\n",
      "Train status:\n",
      "Epochs:  10\n",
      "Repeat(Round):  1\n",
      "Model: \"functional_1\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_layer_1 (InputLayer)      [(None, 100, 1)]     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "zero_padding1d (ZeroPadding1D)  (None, 100, 1)       0           input_layer_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "locally_connected1d (LocallyCon (None, 10, 12)       1200        zero_padding1d[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, 10, 12)       156         locally_connected1d[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "multi_head_qkv__block_attention ((None, 10, 12), (No 432         dense[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "add (Add)                       (None, 10, 12)       0           multi_head_qkv__block_attention[0\n",
      "                                                                 dense[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization (LayerNorma (None, 10, 12)       24          add[0][0]                        \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 10, 12)       156         layer_normalization[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "add_1 (Add)                     (None, 10, 12)       0           dense_1[0][0]                    \n",
      "                                                                 layer_normalization[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "re_lu (ReLU)                    multiple             0           add_1[0][0]                      \n",
      "                                                                 add_3[0][0]                      \n",
      "                                                                 add_4[0][0]                      \n",
      "                                                                 dense_5[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "multi_head_qkv__block_attention ((None, 10, 12), (No 432         re_lu[8][0]                      \n",
      "                                                                 multi_head_qkv__block_attention[0\n",
      "__________________________________________________________________________________________________\n",
      "add_2 (Add)                     (None, 10, 12)       0           multi_head_qkv__block_attention_1\n",
      "                                                                 re_lu[8][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_1 (LayerNor (None, 10, 12)       24          add_2[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 10, 12)       156         layer_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "add_3 (Add)                     (None, 10, 12)       0           dense_2[0][0]                    \n",
      "                                                                 layer_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "flatten (Flatten)               (None, 120)          0           re_lu[9][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "dropout (Dropout)               (None, 120)          0           flatten[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, 64)           7744        dropout[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_4 (Dense)                 (None, 64)           4160        dense_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "add_4 (Add)                     (None, 64)           0           dense_3[0][0]                    \n",
      "                                                                 dense_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_5 (Dense)                 (None, 64)           4160        re_lu[10][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "dense_6 (Dense)                 (None, 1)            65          re_lu[11][0]                     \n",
      "==================================================================================================\n",
      "Total params: 18,709\n",
      "Trainable params: 18,709\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "None\n",
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 1/98 [..............................] - ETA: 0s - loss: 28.3731\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r",
      " 3/98 [..............................] - ETA: 2s - loss: 27.1159"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\pc\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\python\\data\\ops\\dataset_ops.py:3350: UserWarning: Even though the tf.config.experimental_run_functions_eagerly option is set, this option does not apply to tf.data functions. tf.data functions are still traced and executed as graphs.\n",
      "  \"Even though the tf.config.experimental_run_functions_eagerly \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "98/98 [==============================] - 4s 39ms/step - loss: 8.4840 - val_loss: 4.9416\n",
      "Epoch 2/10\n",
      "98/98 [==============================] - 4s 37ms/step - loss: 5.5889 - val_loss: 4.8251\n",
      "Epoch 3/10\n",
      "98/98 [==============================] - 4s 40ms/step - loss: 5.2774 - val_loss: 4.7983\n",
      "Epoch 4/10\n",
      "98/98 [==============================] - 4s 37ms/step - loss: 5.2932 - val_loss: 4.8045\n",
      "Epoch 5/10\n",
      "98/98 [==============================] - 4s 38ms/step - loss: 5.1997 - val_loss: 4.7865\n",
      "Epoch 6/10\n",
      "98/98 [==============================] - 4s 40ms/step - loss: 5.1848 - val_loss: 4.8081\n",
      "Epoch 7/10\n",
      "98/98 [==============================] - 4s 37ms/step - loss: 5.0850 - val_loss: 4.7909\n",
      "Epoch 8/10\n",
      "98/98 [==============================] - 4s 38ms/step - loss: 5.0846 - val_loss: 4.7800\n",
      "Epoch 9/10\n",
      "98/98 [==============================] - 4s 39ms/step - loss: 5.0658 - val_loss: 4.7859\n",
      "Epoch 10/10\n",
      "98/98 [==============================] - 4s 38ms/step - loss: 4.9597 - val_loss: 4.8298\n",
      " - train loss     : 4.959650993347168\n",
      " - validation loss: 4.829834461212158\n",
      " - loss decrease rate in last 5 epochs: 0.00700831413269043\n",
      " - Actual Training epochs:  10\n",
      "Train End.\n",
      "In-year accuracy (measured as Pearson's correlation) is:  0.3372872208493546\n",
      "Training Runtime:  0.6333333333333333  min\n",
      "Checking memory usage is not currently available.\n",
      "USE Attention CNN MODEL as training method\n",
      "All the SNPs are already decoded, imputing missing SNPs with 0.01\n",
      "Convert data to np.array float32\n",
      "The transformed SNP shape: (829, 100, 1)\n",
      "Predicting valid set..\n",
      "Testing prediction:\n",
      "Predicted:  [4.2393694 3.5019722 3.5443947 4.4539227 3.3171532 3.4624372 4.2926145\n",
      " 3.6419823 3.635356  4.036133 ]\n",
      "observed:  [5. 1. 2. 6. 1. 4. 1. 2. 8. 5.]\n",
      "Validate prediction accuracy (measured as Pearson's correlation) is:  0.11653576827680734\n",
      "Saving the model with higher accuracy...\n",
      "Saving model failed, tring directly save by using self._model[\"TRAINED_MODEL\"].save\n",
      "Resource check:\n",
      "Total memory: 31.9502 GB\n",
      "Currently using memory: 2.2782 GB\n",
      "Ratio of used memory: 71.8000  %\n",
      "Number of CPU node:  16\n",
      "Plotting the training process...\n",
      "E:\\learning resource\\OneDrive - The University of Queensland\\PhD\\HPC_Results\\Sugarcane_disease\\ML\\Attention_Test_win/smut_MultiHeadAttentionLNN_smut\n",
      "Plotting loss history...\n",
      "Plot name:  E:\\learning resource\\OneDrive - The University of Queensland\\PhD\\HPC_Results\\Sugarcane_disease\\ML\\Attention_Test_win/smut_MultiHeadAttentionLNN_smut/smut_MultiHeadAttentionLNN_1.png\n",
      "Result:\n",
      "  Trait      TrainSet ValidSet                  Model  Test_Accuracy  \\\n",
      "0  smut  [2, 3, 4, 5]      [1]  MultiHeadAttentionLNN       0.335452   \n",
      "1  smut  [2, 3, 4, 5]      [1]  MultiHeadAttentionLNN       0.315175   \n",
      "2  smut  [1, 3, 4, 5]      [2]  MultiHeadAttentionLNN       0.337287   \n",
      "\n",
      "   Valid_Accuracy       MSE   Runtime  \n",
      "0        0.124699  3.886652  0.716667  \n",
      "1        0.073362  3.922075  0.650000  \n",
      "2        0.116536  4.829834  0.633333  \n",
      "Cross-validate: 3\n",
      "Overall population: 4702\n",
      "759 individuals need to be removed due to the miss phenotype\n",
      "Filtered population: 3943\n",
      "Mean of train phenotype: 4.510671688637791\n",
      "Use raw phenotype as the target\n",
      "12    8.0\n",
      "13    6.0\n",
      "14    5.0\n",
      "19    4.0\n",
      "22    1.0\n",
      "Name: 2, dtype: float64\n",
      "USE Attention CNN MODEL as training method\n",
      "All the SNPs are already decoded, imputing missing SNPs with 0.01\n",
      "Convert data to np.array float32\n",
      "The transformed SNP shape: (3186, 100, 1)\n",
      "USE Attention CNN MODEL as training method\n",
      "All the SNPs are already decoded, imputing missing SNPs with 0.01\n",
      "Convert data to np.array float32\n",
      "The transformed SNP shape: (757, 100, 1)\n",
      "Train status:\n",
      "Epochs:  10\n",
      "Repeat(Round):  1\n",
      "Model: \"functional_1\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_layer_1 (InputLayer)      [(None, 100, 1)]     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "zero_padding1d (ZeroPadding1D)  (None, 100, 1)       0           input_layer_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "locally_connected1d (LocallyCon (None, 10, 12)       1200        zero_padding1d[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, 10, 12)       156         locally_connected1d[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "multi_head_qkv__block_attention ((None, 10, 12), (No 432         dense[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "add (Add)                       (None, 10, 12)       0           multi_head_qkv__block_attention[0\n",
      "                                                                 dense[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization (LayerNorma (None, 10, 12)       24          add[0][0]                        \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 10, 12)       156         layer_normalization[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "add_1 (Add)                     (None, 10, 12)       0           dense_1[0][0]                    \n",
      "                                                                 layer_normalization[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "re_lu (ReLU)                    multiple             0           add_1[0][0]                      \n",
      "                                                                 add_3[0][0]                      \n",
      "                                                                 add_4[0][0]                      \n",
      "                                                                 dense_5[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "multi_head_qkv__block_attention ((None, 10, 12), (No 432         re_lu[12][0]                     \n",
      "                                                                 multi_head_qkv__block_attention[0\n",
      "__________________________________________________________________________________________________\n",
      "add_2 (Add)                     (None, 10, 12)       0           multi_head_qkv__block_attention_1\n",
      "                                                                 re_lu[12][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_1 (LayerNor (None, 10, 12)       24          add_2[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 10, 12)       156         layer_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "add_3 (Add)                     (None, 10, 12)       0           dense_2[0][0]                    \n",
      "                                                                 layer_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "flatten (Flatten)               (None, 120)          0           re_lu[13][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "dropout (Dropout)               (None, 120)          0           flatten[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, 64)           7744        dropout[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_4 (Dense)                 (None, 64)           4160        dense_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "add_4 (Add)                     (None, 64)           0           dense_3[0][0]                    \n",
      "                                                                 dense_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_5 (Dense)                 (None, 64)           4160        re_lu[14][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "dense_6 (Dense)                 (None, 1)            65          re_lu[15][0]                     \n",
      "==================================================================================================\n",
      "Total params: 18,709\n",
      "Trainable params: 18,709\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "None\n",
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  1/100 [..............................] - ETA: 0s - loss: 9.8933\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r",
      "  3/100 [..............................] - ETA: 2s - loss: 9.1319"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\pc\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\python\\data\\ops\\dataset_ops.py:3350: UserWarning: Even though the tf.config.experimental_run_functions_eagerly option is set, this option does not apply to tf.data functions. tf.data functions are still traced and executed as graphs.\n",
      "  \"Even though the tf.config.experimental_run_functions_eagerly \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100/100 [==============================] - 4s 40ms/step - loss: 6.1891 - val_loss: 5.7054\n",
      "Epoch 2/10\n",
      "100/100 [==============================] - 4s 37ms/step - loss: 5.5004 - val_loss: 5.4353\n",
      "Epoch 3/10\n",
      "100/100 [==============================] - 4s 39ms/step - loss: 5.2828 - val_loss: 5.3555\n",
      "Epoch 4/10\n",
      "100/100 [==============================] - 4s 36ms/step - loss: 5.2555 - val_loss: 5.3410\n",
      "Epoch 5/10\n",
      "100/100 [==============================] - 4s 38ms/step - loss: 5.1473 - val_loss: 5.4909\n",
      "Epoch 6/10\n",
      "100/100 [==============================] - 4s 38ms/step - loss: 5.0904 - val_loss: 5.5694\n",
      "Epoch 7/10\n",
      "100/100 [==============================] - 4s 36ms/step - loss: 5.0245 - val_loss: 5.2674\n",
      "Epoch 8/10\n",
      "100/100 [==============================] - 4s 38ms/step - loss: 5.0129 - val_loss: 5.3859\n",
      "Epoch 9/10\n",
      "100/100 [==============================] - 4s 38ms/step - loss: 4.9365 - val_loss: 5.4682\n",
      "Epoch 10/10\n",
      "100/100 [==============================] - 4s 37ms/step - loss: 4.8628 - val_loss: 5.5372\n",
      " - train loss     : 4.862772464752197\n",
      " - validation loss: 5.537170886993408\n",
      " - loss decrease rate in last 5 epochs: -0.02973480224609375\n",
      " - Actual Training epochs:  10\n",
      "Train End.\n",
      "In-year accuracy (measured as Pearson's correlation) is:  0.3241741419179424\n",
      "Training Runtime:  0.6333333333333333  min\n",
      "Checking memory usage is not currently available.\n",
      "USE Attention CNN MODEL as training method\n",
      "All the SNPs are already decoded, imputing missing SNPs with 0.01\n",
      "Convert data to np.array float32\n",
      "The transformed SNP shape: (757, 100, 1)\n",
      "Predicting valid set..\n",
      "Testing prediction:\n",
      "Predicted:  [4.805988  4.4239626 4.546278  4.6252747 3.4293425 4.063488  3.9764059\n",
      " 4.7739377 3.8154068 5.0151787]\n",
      "observed:  [8. 6. 5. 4. 1. 1. 1. 8. 8. 1.]\n",
      "Validate prediction accuracy (measured as Pearson's correlation) is:  0.1664018263516231\n",
      "Saving the model with higher accuracy...\n",
      "Saving model failed, tring directly save by using self._model[\"TRAINED_MODEL\"].save\n",
      "Resource check:\n",
      "Total memory: 31.9502 GB\n",
      "Currently using memory: 2.2840 GB\n",
      "Ratio of used memory: 70.8000  %\n",
      "Number of CPU node:  16\n",
      "Plotting the training process...\n",
      "E:\\learning resource\\OneDrive - The University of Queensland\\PhD\\HPC_Results\\Sugarcane_disease\\ML\\Attention_Test_win/smut_MultiHeadAttentionLNN_smut\n",
      "Plotting loss history...\n",
      "Plot name:  E:\\learning resource\\OneDrive - The University of Queensland\\PhD\\HPC_Results\\Sugarcane_disease\\ML\\Attention_Test_win/smut_MultiHeadAttentionLNN_smut/smut_MultiHeadAttentionLNN_1.png\n",
      "Result:\n",
      "  Trait      TrainSet ValidSet                  Model  Test_Accuracy  \\\n",
      "0  smut  [2, 3, 4, 5]      [1]  MultiHeadAttentionLNN       0.335452   \n",
      "1  smut  [2, 3, 4, 5]      [1]  MultiHeadAttentionLNN       0.315175   \n",
      "2  smut  [1, 3, 4, 5]      [2]  MultiHeadAttentionLNN       0.337287   \n",
      "3  smut  [1, 2, 4, 5]      [3]  MultiHeadAttentionLNN       0.324174   \n",
      "\n",
      "   Valid_Accuracy       MSE   Runtime  \n",
      "0        0.124699  3.886652  0.716667  \n",
      "1        0.073362  3.922075  0.650000  \n",
      "2        0.116536  4.829834  0.633333  \n",
      "3        0.166402  5.537171  0.633333  \n",
      "Cross-validate: 4\n",
      "Overall population: 4702\n",
      "759 individuals need to be removed due to the miss phenotype\n",
      "Filtered population: 3943\n",
      "Mean of train phenotype: 4.519763460939932\n",
      "Use raw phenotype as the target\n",
      "15    5.0\n",
      "17    3.0\n",
      "20    3.0\n",
      "21    2.0\n",
      "25    2.0\n",
      "Name: 2, dtype: float64\n",
      "USE Attention CNN MODEL as training method\n",
      "All the SNPs are already decoded, imputing missing SNPs with 0.01\n",
      "Convert data to np.array float32\n",
      "The transformed SNP shape: (3213, 100, 1)\n",
      "USE Attention CNN MODEL as training method\n",
      "All the SNPs are already decoded, imputing missing SNPs with 0.01\n",
      "Convert data to np.array float32\n",
      "The transformed SNP shape: (730, 100, 1)\n",
      "Train status:\n",
      "Epochs:  10\n",
      "Repeat(Round):  1\n",
      "Model: \"functional_1\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_layer_1 (InputLayer)      [(None, 100, 1)]     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "zero_padding1d (ZeroPadding1D)  (None, 100, 1)       0           input_layer_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "locally_connected1d (LocallyCon (None, 10, 12)       1200        zero_padding1d[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, 10, 12)       156         locally_connected1d[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "multi_head_qkv__block_attention ((None, 10, 12), (No 432         dense[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "add (Add)                       (None, 10, 12)       0           multi_head_qkv__block_attention[0\n",
      "                                                                 dense[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization (LayerNorma (None, 10, 12)       24          add[0][0]                        \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 10, 12)       156         layer_normalization[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "add_1 (Add)                     (None, 10, 12)       0           dense_1[0][0]                    \n",
      "                                                                 layer_normalization[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "re_lu (ReLU)                    multiple             0           add_1[0][0]                      \n",
      "                                                                 add_3[0][0]                      \n",
      "                                                                 add_4[0][0]                      \n",
      "                                                                 dense_5[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "multi_head_qkv__block_attention ((None, 10, 12), (No 432         re_lu[16][0]                     \n",
      "                                                                 multi_head_qkv__block_attention[0\n",
      "__________________________________________________________________________________________________\n",
      "add_2 (Add)                     (None, 10, 12)       0           multi_head_qkv__block_attention_1\n",
      "                                                                 re_lu[16][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_1 (LayerNor (None, 10, 12)       24          add_2[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 10, 12)       156         layer_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "add_3 (Add)                     (None, 10, 12)       0           dense_2[0][0]                    \n",
      "                                                                 layer_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "flatten (Flatten)               (None, 120)          0           re_lu[17][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "dropout (Dropout)               (None, 120)          0           flatten[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, 64)           7744        dropout[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_4 (Dense)                 (None, 64)           4160        dense_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "add_4 (Add)                     (None, 64)           0           dense_3[0][0]                    \n",
      "                                                                 dense_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_5 (Dense)                 (None, 64)           4160        re_lu[18][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "dense_6 (Dense)                 (None, 1)            65          re_lu[19][0]                     \n",
      "==================================================================================================\n",
      "Total params: 18,709\n",
      "Trainable params: 18,709\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "None\n",
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  1/101 [..............................] - ETA: 0s - loss: 37.5264\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r",
      "  3/101 [..............................] - ETA: 2s - loss: 29.7586"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\pc\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\python\\data\\ops\\dataset_ops.py:3350: UserWarning: Even though the tf.config.experimental_run_functions_eagerly option is set, this option does not apply to tf.data functions. tf.data functions are still traced and executed as graphs.\n",
      "  \"Even though the tf.config.experimental_run_functions_eagerly \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "101/101 [==============================] - 4s 38ms/step - loss: 11.4657 - val_loss: 5.4542\n",
      "Epoch 2/10\n",
      "101/101 [==============================] - 4s 37ms/step - loss: 5.5944 - val_loss: 5.3850\n",
      "Epoch 3/10\n",
      "101/101 [==============================] - 4s 38ms/step - loss: 5.3217 - val_loss: 5.3307\n",
      "Epoch 4/10\n",
      "101/101 [==============================] - 4s 37ms/step - loss: 5.1715 - val_loss: 5.2930\n",
      "Epoch 5/10\n",
      "101/101 [==============================] - 4s 38ms/step - loss: 5.0380 - val_loss: 5.3660\n",
      "Epoch 6/10\n",
      "101/101 [==============================] - 4s 39ms/step - loss: 4.9494 - val_loss: 5.4032\n",
      "Epoch 7/10\n",
      "101/101 [==============================] - 4s 36ms/step - loss: 5.0033 - val_loss: 5.3608\n",
      "Epoch 8/10\n",
      "101/101 [==============================] - 4s 38ms/step - loss: 4.8810 - val_loss: 5.2128\n",
      "Epoch 9/10\n",
      "101/101 [==============================] - 4s 37ms/step - loss: 4.7645 - val_loss: 5.1764\n",
      "Epoch 10/10\n",
      "101/101 [==============================] - 4s 36ms/step - loss: 4.9266 - val_loss: 5.2496\n",
      " - train loss     : 4.926567077636719\n",
      " - validation loss: 5.249615669250488\n",
      " - loss decrease rate in last 5 epochs: -0.027637338638305663\n",
      " - Actual Training epochs:  10\n",
      "Train End.\n",
      "In-year accuracy (measured as Pearson's correlation) is:  0.34643776480482236\n",
      "Training Runtime:  0.65  min\n",
      "Checking memory usage is not currently available.\n",
      "USE Attention CNN MODEL as training method\n",
      "All the SNPs are already decoded, imputing missing SNPs with 0.01\n",
      "Convert data to np.array float32\n",
      "The transformed SNP shape: (730, 100, 1)\n",
      "Predicting valid set..\n",
      "Testing prediction:\n",
      "Predicted:  [4.884761  4.3123913 4.7440114 4.127284  3.2981188 3.1925788 3.8240905\n",
      " 4.7060175 4.2266097 3.6704602]\n",
      "observed:  [5. 3. 3. 2. 2. 5. 5. 4. 7. 5.]\n",
      "Validate prediction accuracy (measured as Pearson's correlation) is:  0.18484319723612389\n",
      "Saving the model with higher accuracy...\n",
      "Saving model failed, tring directly save by using self._model[\"TRAINED_MODEL\"].save\n",
      "Resource check:\n",
      "Total memory: 31.9502 GB\n",
      "Currently using memory: 2.2888 GB\n",
      "Ratio of used memory: 70.8000  %\n",
      "Number of CPU node:  16\n",
      "Plotting the training process...\n",
      "E:\\learning resource\\OneDrive - The University of Queensland\\PhD\\HPC_Results\\Sugarcane_disease\\ML\\Attention_Test_win/smut_MultiHeadAttentionLNN_smut\n",
      "Plotting loss history...\n",
      "Plot name:  E:\\learning resource\\OneDrive - The University of Queensland\\PhD\\HPC_Results\\Sugarcane_disease\\ML\\Attention_Test_win/smut_MultiHeadAttentionLNN_smut/smut_MultiHeadAttentionLNN_1.png\n",
      "Result:\n",
      "  Trait      TrainSet ValidSet                  Model  Test_Accuracy  \\\n",
      "0  smut  [2, 3, 4, 5]      [1]  MultiHeadAttentionLNN       0.335452   \n",
      "1  smut  [2, 3, 4, 5]      [1]  MultiHeadAttentionLNN       0.315175   \n",
      "2  smut  [1, 3, 4, 5]      [2]  MultiHeadAttentionLNN       0.337287   \n",
      "3  smut  [1, 2, 4, 5]      [3]  MultiHeadAttentionLNN       0.324174   \n",
      "4  smut  [1, 2, 3, 5]      [4]  MultiHeadAttentionLNN       0.346438   \n",
      "\n",
      "   Valid_Accuracy       MSE   Runtime  \n",
      "0        0.124699  3.886652  0.716667  \n",
      "1        0.073362  3.922075  0.650000  \n",
      "2        0.116536  4.829834  0.633333  \n",
      "3        0.166402  5.537171  0.633333  \n",
      "4        0.184843  5.249616  0.650000  \n",
      "Cross-validate: 5\n",
      "Overall population: 4702\n",
      "759 individuals need to be removed due to the miss phenotype\n",
      "Filtered population: 3943\n",
      "Mean of train phenotype: 4.327210460772105\n",
      "Use raw phenotype as the target\n",
      "26     1.0\n",
      "80     6.0\n",
      "85     4.0\n",
      "96     6.0\n",
      "210    3.0\n",
      "Name: 2, dtype: float64\n",
      "USE Attention CNN MODEL as training method\n",
      "All the SNPs are already decoded, imputing missing SNPs with 0.01\n",
      "Convert data to np.array float32\n",
      "The transformed SNP shape: (3212, 100, 1)\n",
      "USE Attention CNN MODEL as training method\n",
      "All the SNPs are already decoded, imputing missing SNPs with 0.01\n",
      "Convert data to np.array float32\n",
      "The transformed SNP shape: (731, 100, 1)\n",
      "Train status:\n",
      "Epochs:  10\n",
      "Repeat(Round):  1\n",
      "Model: \"functional_1\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_layer_1 (InputLayer)      [(None, 100, 1)]     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "zero_padding1d (ZeroPadding1D)  (None, 100, 1)       0           input_layer_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "locally_connected1d (LocallyCon (None, 10, 12)       1200        zero_padding1d[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, 10, 12)       156         locally_connected1d[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "multi_head_qkv__block_attention ((None, 10, 12), (No 432         dense[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "add (Add)                       (None, 10, 12)       0           multi_head_qkv__block_attention[0\n",
      "                                                                 dense[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization (LayerNorma (None, 10, 12)       24          add[0][0]                        \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 10, 12)       156         layer_normalization[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "add_1 (Add)                     (None, 10, 12)       0           dense_1[0][0]                    \n",
      "                                                                 layer_normalization[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "re_lu (ReLU)                    multiple             0           add_1[0][0]                      \n",
      "                                                                 add_3[0][0]                      \n",
      "                                                                 add_4[0][0]                      \n",
      "                                                                 dense_5[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "multi_head_qkv__block_attention ((None, 10, 12), (No 432         re_lu[20][0]                     \n",
      "                                                                 multi_head_qkv__block_attention[0\n",
      "__________________________________________________________________________________________________\n",
      "add_2 (Add)                     (None, 10, 12)       0           multi_head_qkv__block_attention_1\n",
      "                                                                 re_lu[20][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_1 (LayerNor (None, 10, 12)       24          add_2[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 10, 12)       156         layer_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "add_3 (Add)                     (None, 10, 12)       0           dense_2[0][0]                    \n",
      "                                                                 layer_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "flatten (Flatten)               (None, 120)          0           re_lu[21][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "dropout (Dropout)               (None, 120)          0           flatten[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, 64)           7744        dropout[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_4 (Dense)                 (None, 64)           4160        dense_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "add_4 (Add)                     (None, 64)           0           dense_3[0][0]                    \n",
      "                                                                 dense_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_5 (Dense)                 (None, 64)           4160        re_lu[22][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "dense_6 (Dense)                 (None, 1)            65          re_lu[23][0]                     \n",
      "==================================================================================================\n",
      "Total params: 18,709\n",
      "Trainable params: 18,709\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "None\n",
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  1/101 [..............................] - ETA: 0s - loss: 47.8123\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r",
      "  3/101 [..............................] - ETA: 2s - loss: 40.7736"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\pc\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\python\\data\\ops\\dataset_ops.py:3350: UserWarning: Even though the tf.config.experimental_run_functions_eagerly option is set, this option does not apply to tf.data functions. tf.data functions are still traced and executed as graphs.\n",
      "  \"Even though the tf.config.experimental_run_functions_eagerly \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "101/101 [==============================] - 4s 39ms/step - loss: 13.8788 - val_loss: 6.7555\n",
      "Epoch 2/10\n",
      "101/101 [==============================] - 4s 38ms/step - loss: 5.4021 - val_loss: 6.6874\n",
      "Epoch 3/10\n",
      "101/101 [==============================] - 4s 38ms/step - loss: 5.1933 - val_loss: 6.7546\n",
      "Epoch 4/10\n",
      "101/101 [==============================] - 4s 37ms/step - loss: 5.0380 - val_loss: 6.8453\n",
      "Epoch 5/10\n",
      "101/101 [==============================] - 4s 38ms/step - loss: 4.9660 - val_loss: 6.7363\n",
      "Epoch 6/10\n",
      "101/101 [==============================] - 4s 37ms/step - loss: 4.9407 - val_loss: 6.8356\n",
      "Epoch 7/10\n",
      "101/101 [==============================] - 4s 37ms/step - loss: 4.8966 - val_loss: 6.2015\n",
      "Epoch 8/10\n",
      "101/101 [==============================] - 4s 37ms/step - loss: 4.8460 - val_loss: 6.8832\n",
      "Epoch 9/10\n",
      "101/101 [==============================] - 4s 37ms/step - loss: 4.8163 - val_loss: 6.2618\n",
      "Epoch 10/10\n",
      "101/101 [==============================] - 4s 36ms/step - loss: 4.7266 - val_loss: 6.1995\n",
      " - train loss     : 4.726583957672119\n",
      " - validation loss: 6.199548721313477\n",
      " - loss decrease rate in last 5 epochs: -0.19685840606689453\n",
      " - Actual Training epochs:  10\n",
      "Train End.\n",
      "In-year accuracy (measured as Pearson's correlation) is:  0.25461255667529775\n",
      "Training Runtime:  0.65  min\n",
      "Checking memory usage is not currently available.\n",
      "USE Attention CNN MODEL as training method\n",
      "All the SNPs are already decoded, imputing missing SNPs with 0.01\n",
      "Convert data to np.array float32\n",
      "The transformed SNP shape: (731, 100, 1)\n",
      "Predicting valid set..\n",
      "Testing prediction:\n",
      "Predicted:  [3.9740207 4.211191  5.01759   4.6254272 4.7021236 4.626583  4.5146174\n",
      " 4.788496  4.5184464 5.0658073]\n",
      "observed:  [1. 6. 4. 6. 3. 6. 9. 8. 3. 9.]\n",
      "Validate prediction accuracy (measured as Pearson's correlation) is:  0.25568799493917505\n",
      "Saving the model with higher accuracy...\n",
      "Saving model failed, tring directly save by using self._model[\"TRAINED_MODEL\"].save\n",
      "Resource check:\n",
      "Total memory: 31.9502 GB\n",
      "Currently using memory: 2.2942 GB\n",
      "Ratio of used memory: 71.1000  %\n",
      "Number of CPU node:  16\n",
      "Plotting the training process...\n",
      "E:\\learning resource\\OneDrive - The University of Queensland\\PhD\\HPC_Results\\Sugarcane_disease\\ML\\Attention_Test_win/smut_MultiHeadAttentionLNN_smut\n",
      "Plotting loss history...\n",
      "Plot name:  E:\\learning resource\\OneDrive - The University of Queensland\\PhD\\HPC_Results\\Sugarcane_disease\\ML\\Attention_Test_win/smut_MultiHeadAttentionLNN_smut/smut_MultiHeadAttentionLNN_1.png\n",
      "Result:\n",
      "  Trait      TrainSet ValidSet                  Model  Test_Accuracy  \\\n",
      "0  smut  [2, 3, 4, 5]      [1]  MultiHeadAttentionLNN       0.335452   \n",
      "1  smut  [2, 3, 4, 5]      [1]  MultiHeadAttentionLNN       0.315175   \n",
      "2  smut  [1, 3, 4, 5]      [2]  MultiHeadAttentionLNN       0.337287   \n",
      "3  smut  [1, 2, 4, 5]      [3]  MultiHeadAttentionLNN       0.324174   \n",
      "4  smut  [1, 2, 3, 5]      [4]  MultiHeadAttentionLNN       0.346438   \n",
      "5  smut  [1, 2, 3, 4]      [5]  MultiHeadAttentionLNN       0.254613   \n",
      "\n",
      "   Valid_Accuracy       MSE   Runtime  \n",
      "0        0.124699  3.886652  0.716667  \n",
      "1        0.073362  3.922075  0.650000  \n",
      "2        0.116536  4.829834  0.633333  \n",
      "3        0.166402  5.537171  0.633333  \n",
      "4        0.184843  5.249616  0.650000  \n",
      "5        0.255688  6.199549  0.650000  \n"
     ]
    }
   ],
   "source": [
    "index_ref = composer.prepare_cross_validate()\n",
    "i = 1\n",
    "for train_idx,valid_idx in index_ref:\n",
    "    print(\"Cross-validate: {}\".format(i))\n",
    "    composer.prepare_training(train_idx,valid_idx)\n",
    "    composer.compose(train_idx,valid_idx)\n",
    "    i+=1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
