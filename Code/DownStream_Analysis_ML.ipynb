{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "import glob     #for checking dir content\n",
    "import os       #for dir creation\n",
    "from Functions import *\n",
    "from GSModel import *\n",
    "import pandas as pd\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import MaxPooling1D, Flatten, Dense, Conv1D,MaxPooling2D, Conv2D\n",
    "from keras.layers import Dropout\n",
    "import keras.metrics\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.utils import to_categorical\n",
    "import argparse\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import platform\n",
    "from datetime import datetime\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import configparser\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "./DS_config.ini\n",
      "Using backup path (for trouble shooting)\n",
      "E:\\learning resource\\PhD\\genomic data\\Sugarcane\\qc_genotypes.csv\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "#Load genomic data and phenotypes\n",
    "config_path = \"./DS_config.ini\"\n",
    "config = configparser.ConfigParser()\n",
    "if platform.system().lower() == \"windows\":\n",
    "    print(config_path)\n",
    "    config.read(config_path)\n",
    "else:\n",
    "    config.read(config_path)\n",
    "\n",
    "try:\n",
    "    geno_data = pd.read_csv(config[\"PATH\"][\"genotype\"], sep=\"\\t\")  # pd.read_csv(\"../fitted_genos.csv\",sep=\"\\t\")\n",
    "    pheno_data = pd.read_csv(config[\"PATH\"][\"phenotype\"], sep=\"\\t\")  # pd.read_csv(\"../phenotypes.csv\",sep=\"\\t\")\n",
    "except:\n",
    "    try:\n",
    "        print(\"Using backup path (for trouble shooting)\")\n",
    "        print(config[\"BACKUP_PATH\"][\"genotype\"])\n",
    "        geno_data = pd.read_csv(config[\"BACKUP_PATH\"][\"genotype\"],\n",
    "                                sep=\"\\t\")  # pd.read_csv(\"../fitted_genos.csv\",sep=\"\\t\")\n",
    "        pheno_data = pd.read_csv(config[\"BACKUP_PATH\"][\"phenotype\"],\n",
    "                                    sep=\"\\t\")  # pd.read_csv(\"../phenotypes.csv\",sep=\"\\t\")\n",
    "    except:\n",
    "        print(\"No valid path found.\")\n",
    "        exit()\n",
    "geno_data = decoding(geno_data)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "E:/learning resource/PhD/HPC_Results/CNN/backup/2013-2015_vs_2017/models/TCHBlup_CNN_all_model.json.h5\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "path = \"E:/learning resource/PhD/HPC_Results/CNN/backup/2013-2015_vs_2017/models/\"\n",
    "trait_name = \"TCHBlup\"\n",
    "region = \"all\"\n",
    "method = \"CNN\"\n",
    "model_path = path + \"{}_{}_{}_model.json\".format(trait_name,method,region)\n",
    "weights_path = model_path + \".h5\"\n",
    "print(weights_path)\n",
    "#region_index = \"all\"\n",
    "#trait = \"TCHBlup\"\n",
    "f_json = open(model_path,\"r\")\n",
    "model_json = f_json.read()\n",
    "f_json.close()\n",
    "trained_model = keras.models.model_from_json(model_json)\n",
    "trained_model.load_weights(weights_path)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Process feature data"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "Detected non-genetic factors from phenotype file:  ['Series', 'Region', 'Trial', 'Crop', 'Clone']\n",
      "Got selected years: [2013, 2014, 2015, 2017]\n",
      "Removing useless non-genetic factors: ['Trial', 'Crop', 'Clone', 'Sample']\n",
      "Index(['Series', 'CCSBlup', 'TCHBlup', 'FibreBlup', '0', '1', '2', '3', '4',\n",
      "       '5',\n",
      "       ...\n",
      "       '26076', '26077', '26078', '26079', '26080', '26081', '26082', '26083',\n",
      "       '26084', '26085'],\n",
      "      dtype='object', length=26090)\n",
      "Index(['0', '1', '2', '3', '4', '5', '6', '7', '8', '9',\n",
      "       ...\n",
      "       '26076', '26077', '26078', '26079', '26080', '26081', '26082', '26083',\n",
      "       '26084', '26085'],\n",
      "      dtype='object', length=26086)\n",
      "(1200, 26086)\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "\"\"\"\n",
    "traits = config[\"BASIC\"][\"traits\"]\n",
    "train_year = config[\"BASIC\"][\"train\"]\n",
    "valid_year = config[\"BASIC\"][\"valid\"]\n",
    "\n",
    "filtered_data = read_pipes(geno_data, pheno_data, train_year + valid_year)\n",
    "non_genetic_factors = [x for x in pheno_data.columns if x not in traits]\n",
    "print(\"Detected non-genetic factors from phenotype file: \",non_genetic_factors)\n",
    "\n",
    "dropout = [x for x in non_genetic_factors if\n",
    "                   x not in [\"Region\",\"Series\"]] + [\"Sample\"] # config[\"BASIC\"][\"drop\"].split(\"#\") + ['Sample']\n",
    "print(\"Removing useless non-genetic factors: {}\".format(dropout))\n",
    "filtered_data.drop(dropout, axis=1, inplace=True)\n",
    "\"\"\"\n",
    "def select_subset(config,geno,pheno,select_by):\n",
    "    traits = config[\"BASIC\"][\"traits\"]\n",
    "    train_year = get_years(config[\"BASIC\"][\"train\"])\n",
    "    valid_year = get_years(config[\"BASIC\"][\"valid\"])\n",
    "    non_genetic_factors = [x for x in pheno.columns if x not in traits]\n",
    "    print(\"Detected non-genetic factors from phenotype file: \", non_genetic_factors)\n",
    "    filtered_data = read_pipes(geno, pheno, train_year + valid_year)\n",
    "    dropout = [x for x in non_genetic_factors if\n",
    "               x not in [\"Region\", \"Series\"]] + [\"Sample\"]  # config[\"BASIC\"][\"drop\"].split(\"#\") + ['Sample']\n",
    "    print(\"Removing useless non-genetic factors: {}\".format(dropout))\n",
    "    filtered_data.drop(dropout, axis=1, inplace=True)\n",
    "\n",
    "    select_data = filtered_data.query('Region in @select_by').drop([\"Region\"],axis=1)\n",
    "\n",
    "    return select_data\n",
    "\n",
    "region_set = pheno_data.Region.unique()\n",
    "if region == \"all\":\n",
    "    region_index = region_set\n",
    "train_year = get_years(config[\"BASIC\"][\"train\"])\n",
    "valid_year = get_years(config[\"BASIC\"][\"valid\"])\n",
    "selected_data = select_subset(config,geno_data,pheno_data,region_index)\n",
    "drop_traits = [\"CCSBlup\",\"FibreBlup\"]\n",
    "print(selected_data.columns)\n",
    "selected_data.dropna(subset=[trait_name],axis=0,inplace=True)\n",
    "#print(selected_data.columns)\n",
    "selected_data.drop(drop_traits,axis=1,inplace=True)\n",
    "train_targets = selected_data.query(\"Series in @train_year\")[trait_name]\n",
    "valid_targets = selected_data.query(\"Series in @valid_year\")[trait_name]\n",
    "train_features = selected_data.query(\"Series in @train_year\").drop([\"Series\",trait_name],axis=1)\n",
    "valid_features = selected_data.query(\"Series in @valid_year\").drop([\"Series\",trait_name],axis=1)\n",
    "print(train_features.columns)\n",
    "print(train_features.shape)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "0.3733788069301798\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "X = np.expand_dims(valid_features,axis=2)\n",
    "length = len(valid_targets)\n",
    "outputs = np.reshape(trained_model.predict(X), (length,))\n",
    "#print(outputs)\n",
    "print(np.corrcoef(outputs,valid_targets)[0,1])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "from keras import models\n",
    "layer_outputs = [layer.output for layer in trained_model.layers[:4]]\n",
    "activation_model = models.Model(inputs=trained_model.input, outputs=layer_outputs)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "(400, 724, 128)\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "activations = activation_model.predict(X)\n",
    "first_layer_activation = activations[-1]\n",
    "print(first_layer_activation.shape)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "(1, 400, 724, 128)\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "img_data = np.expand_dims(first_layer_activation,axis=0)\n",
    "print(img_data.shape)\n",
    "figure = plt.figure()\n",
    "axes = figure.add_subplot(111) \n",
    "img = axes.matshow(img_data[0,:,:,-1],cmap='viridis', aspect='auto')\n",
    "figure.colorbar(img) \n",
    "plt.savefig(\"../../HPC_Results/CNN/backup/TCH_all_channel_6.png\",pad_inches=0.4)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "outputs": [
    {
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-62-bf3aff62188b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      9\u001b[0m     \u001b[0msize\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlayer_activation\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;31m#The feature map has shape (1, size, size, n_features).\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m     \u001b[0mn_cols\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mn_features\u001b[0m \u001b[1;33m//\u001b[0m \u001b[0mimages_per_row\u001b[0m \u001b[1;31m# Tiles the activation channels in this matrix\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 11\u001b[1;33m     \u001b[0mdisplay_grid\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msize\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mn_cols\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mimages_per_row\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0msize\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     12\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mcol\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn_cols\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;31m# Tiles each filter into a big horizontal grid\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mrow\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimages_per_row\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mMemoryError\u001b[0m: Unable to allocate 36.0 GiB for an array with shape (34776, 139104) and data type float64"
     ],
     "ename": "MemoryError",
     "evalue": "Unable to allocate 36.0 GiB for an array with shape (34776, 139104) and data type float64",
     "output_type": "error"
    }
   ],
   "source": [
    "layer_names = []\n",
    "for layer in trained_model.layers[:12]:\n",
    "    layer_names.append(layer.name) # Names of the layers, so you can have them as part of your plot\n",
    "    \n",
    "images_per_row = 16\n",
    "\n",
    "for layer_name, layer_activation in zip(layer_names, activations): # Displays the feature maps\n",
    "    n_features = layer_activation.shape[-1] # Number of features in the feature map\n",
    "    size = layer_activation.shape[1] #The feature map has shape (1, size, size, n_features).\n",
    "    n_cols = n_features // images_per_row # Tiles the activation channels in this matrix\n",
    "    display_grid = np.zeros((size * n_cols, images_per_row * size))\n",
    "    for col in range(n_cols): # Tiles each filter into a big horizontal grid\n",
    "        for row in range(images_per_row):\n",
    "            channel_image = layer_activation[0,\n",
    "                                             :, :,\n",
    "                                             col * images_per_row + row]\n",
    "            channel_image -= channel_image.mean() # Post-processes the feature to make it visually palatable\n",
    "            channel_image /= channel_image.std()\n",
    "            channel_image *= 64\n",
    "            channel_image += 128\n",
    "            channel_image = np.clip(channel_image, 0, 255).astype('uint8')\n",
    "            display_grid[col * size : (col + 1) * size, # Displays the grid\n",
    "                         row * size : (row + 1) * size] = channel_image\n",
    "    scale = 1. / size\n",
    "    plt.figure(figsize=(scale * display_grid.shape[1],\n",
    "                        scale * display_grid.shape[0]))\n",
    "    plt.title(layer_name)\n",
    "    plt.grid(False)\n",
    "    plt.imshow(display_grid, aspect='auto', cmap='viridis')\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "pycharm-3bcd6117",
   "language": "python",
   "display_name": "PyCharm (PHD_Notebook)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "source": [],
    "metadata": {
     "collapsed": false
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}