Attention Block prediction



m (m << M) SNPs convert to SNP block $B_n$ 



$Eff_{B_n}$ = $Allele_n * \sum{Epi_i} + bias + Residual$

$Epi_i = alleles_i * weight_i  $

$bias$ -> Dominance effects, etc.



GOALs: Attention Network; Shap investigate

Add LD block to Blocking strategy - maybe different block sizes. (Ask Eric)

Minus means before training

Bayesian posterior information investigate

 

```bash
composer="/afm01/Q4/Q4179/ML_composer/"
ls -lh $composer
cp -r $composer $TMPDIR/
locat=`pwd`
geno="data/sugarcane_disease"
pheno="data/sugarcane_disease.phen"
index="data/sugarcane_disease.index"
target="./MultiHead_Attention"
model="MultiHead Attention LNN"
width=64
depth=0

python $TMPDIR/ML_composer/GS_composer.py --ped $geno --pheno $pheno --mpheno 1 --index $index --trait smut --width $width --depth $depth --model "MultiHead Attention LNN" -o $target --quiet 1 --plot --epoch 15
```

