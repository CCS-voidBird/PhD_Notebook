ToDo List (Feb - Mar 2023)

1. AIGS sugarcane revision 
2. Multi head block attention network
3. Sort results & investigation
4. Make AGBT slides
5. Shap disabled (Conflict with current tensorflow version), considering writing own downstream pipelines



Current Issues:

1. disease DL prediction: high correlation but also high MSE

   

Bayesian investigate

1. posterior inclusion probability (summary Bayesian)
2. 200511_SBayes_Jackie_Kiewa.pptx (details)

Machine learning parental selection
Khaki S, Khalilzadeh Z, Wang L (2020) Predicting yield performance of parents in plant breeding: A neural collaborative filtering approach. PLOS ONE 15(5): e0233382. https://doi.org/10.1371/journal.pone.0233382



Song, W., Shi, C., Xiao, Z., Duan, Z., Xu, Y., Zhang, M., & Tang, J. (2019, November). Autoint: Automatic feature interaction learning via self-attentive neural networks. In *Proceedings of the 28th ACM International Conference on Information and Knowledge Management* (pp. 1161-1170).

![AutoInt:使用Multi-head Self-Attention进行自动特征学习的CTR模型](https://picx.zhimg.com/v2-4875432d62160ab97a87a17d900dd704_720w.jpg?source=172ae18b)

```bash
composer="/afm03/Q4/Q4179/ML_composer/"
ls -lh $composer
cp -r $composer $TMPDIR/
locat=`pwd`
geno="data/sugarcane_disease"
pheno="data/sugarcane_disease.phen"
index="data/sugarcane_disease.index"
anno="data/Annotation/SNPinfo-0.4-4.txt"
target="./Attention_Test"
model="MultiHead Attention LNN"
width=256
depth=0

python $TMPDIR/ML_composer/GS_composer.py --ped $geno --pheno $pheno --mpheno 1 --index $index --annotation $anno --trait smut --width $width --depth $depth --model "MultiHead Attention LNN" -o $target --quiet 0 --plot --epoch 10 --num-heads 2 --batch 2 --lr 0.0001

python $TMPDIR/ML_composer/GS_composer.py --ped $geno --pheno $pheno --mpheno 1 --index $index --trait smut --width $width --depth $depth --model "Attention CNN" -o $target --quiet 1 --plot --epoch 15 --num-heads 2 --lr 0.001 --residual --batch 8

#On submit script
python $TMPDIR/ML_composer/GS_composer.py --ped $geno --pheno $pheno --mpheno 1 --index $index --trait smut --width $width --depth $depth --model "Attention CNN" -o $target --quiet 0 --plot --epoch 80 --num-heads 2 --lr 0.01 --residual
python $TMPDIR/ML_composer/GS_composer.py --ped $geno --pheno $pheno --mpheno 2 --index $index --trait pachy --width $width --depth $depth --model "Attention CNN" -o $target --quiet 0 --plot --epoch 80 --num-heads 2 --lr 0.01 --residual
```



Self designed SNP global attention:

Raw Q,K,V shape: (batch, seq, 1,embed) or (batch, seq, embed)

Wk = (seq,1,embed) or (seq,embed) = 2608xembed 8 

Wv = (seq,embed) x (embed,second_embed) = (seq,second_embed) = 2606xembed + embedxsecondEmbed 8


$$
Attention\ Score (A) = softmax(Q(W_kK)^T)
$$
Output shape: (batch,Seq,Seq,embed) or (batch, seq, seq) 

Optional (Attention convolution) by (embeding//heads,1), filter: second embedding, step:1 -- get shape (batch, seq, seq,second_embeding) = embedding x second Embed= 64

Kernel shape divided by heads?


$$
Effect\ Score (E) = A(W_V \cdot V)
$$
output shape: (batch,seq,seq,second_embed)

Second convolution: kernel (seq, second_mebed), filter, get output shape: (batch, seq, filter) 2608 x 8

1xfilter convolution,1 filter

Global average pooling

get prediction



LD x SNP attention

LD~LD Attention (LDxLD) Individual SNP were grouped as LD 0, others by LD number

​	shape (LD,LD)

Inside-LD Attention using Mask

Estimate GEBV:
$$
GEBV = M_{SNP} * (W_{LD}*W_{LD-SNP}) 
$$
M_SNP = (B, S, LD) (LD from 1~n+1) individual SNP grouped as LD 0

M_LD = (LD,1) (LD from 0~n)

M_LD_SNP = (S, LD) 

Final SNP weight = SNP alleles * LD weight (with attention) * SNP weight~LD (with LD)



Attention drop out layer

 Choe, J., & Shim, H. (2019). Attention-based dropout layer for weakly supervised object localization. In *Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition* (pp. 2219-2228).



Currently running DL tasks:

LNN_SNPMAP_Attention_single 

LNN_MultiHead_Attention (Standard)

LNN_ConvEmbedding_MultiHead_Attention

LNN_SNPMAP_ConvEmbedding_MultiHead_Attention

LD_Attention



```R
library(dplyr)
library(tidyr)
library(stringr)
rs = c(0.1,0.2,0.5,0.7,0.9,1,2,3,4,5)
tors = c(1,2,3,4,5)
for (r in rs){
	actual_r = r/10
	for (tor in tors) {
        filename = paste0("LDs/LD-SNPs-",r,"-",tor,".txt")
        print(filename)
        lds = read.table(filename,sep=" ",h=T)
        lds$LID = rownames(lds)
		lds$Scount = str_count(lds$Markers, ";")
        summary(lds$Scount)
        if (1 %in% lds$Scount){
        	lds[which(lds$Scount == 1),]$LID = 0}
        lds$Markers = as.character(lds$Markers)
        lds %>% 
		mutate(SID = strsplit(Markers,";")) %>% 
		unnest(SID) %>% select(c("SID","LID")) %>% write.table(paste0("SNP_LD_0/SNPinfo-",actual_r,"-",tor,".txt"),row.names=F,quote=F,col.names=T,sep="\t")
    }
}


```

https://openai.com/research/sparse-transformer



Test accuracy dependency for attention model on embedding/blocking channels



New script

```bash
composer="/afm03/Q4/Q4179/ML_composer/"
ls -lh $composer
cp -r $composer $TMPDIR/
locat=`pwd`
geno="data/sugarcane_disease"
pheno="data/sugarcane_disease.phen"
index="data/sugarcane_disease.index"
anno="data/Annotation/SNPinfo-0.05-2.txt"
target="./Attention_Test"
model="MultiHead Attention LNN"
width=256
depth=2

python $TMPDIR/ML_composer/GS_composer.py --ped $geno --pheno $pheno --mpheno 1 --index $index --annotation $anno --trait smut --width $width --depth $depth --model "MultiHead Attention LNN" -o $target --quiet 1 --round 1 --plot --epoch 10 --embedding 64 --num-heads 1 --batch 12 --lr 0.0001 --residual


######using subset########
run
source ~/.bash_profile

locat=`pwd`
geno="data/subset/disease_subset"
pheno="data/subset/disease_subset.phen"
index="data/subset/disease_subset.index"
anno="data/subset/disease_subset.anno"
target="./Attention_Test"
model="MultiHead Attention LNN"
width=256
depth=2
heads=2
composer="/afm03/Q4/Q4179/ML_composer/"
ls -lh $composer
cp -r $composer $TMPDIR/
python $TMPDIR/ML_composer/GS_composer.py --ped $geno --pheno $pheno --mpheno 1 --index $index --annotation $anno --trait smut --width $width --depth $depth --model "MultiHead Attention LNN" -o $target --quiet 1 --round 1 --plot --epoch 10 --embedding 32 --num-heads 2 --batch 64 --lr 0.0001 --residual

```





Check LD training results:

```bash
for i in `ls Single_Head_LD_Residual_attention_realForm32_32Local`
do
echo $i
cat Single_Head_LD_Residual_attention_realForm32_32Local/$i/MultiHeadAttentionLNN_train_record_smut.csv >> LD_show.txt
done 
```



```R

########################
#Get cor for mtg2 results#
#########################

get_auc <- function(bv_name,ref_name,trait_col=1,val_idx=1){
  gebvs = read.table(bv_name,sep="",h=F,skip = 1)
  ref = read.table(ref_name,sep=" ",h=F)
  
  mask = which(ref[,dim(ref)[2]]==val_idx) #Get rows of validate index
  val_phen = gebvs[mask,2]
  val_ref = ref[mask,2+trait_col]
  
  na_rm = which(!is.na(val_ref))  #filter NA values from ref validate rows
  r2 = cor(round(val_phen[na_rm],digits=0),val_ref[na_rm])
  print(trait_col)
  print(val_idx)
  print(length(na_rm))
  print(r2)
  return(r2)
}

#get_auc(bv_name,ref_name,trait_col=1,val_idx=1)

grms = c("a","ad","ade")
traits = c("smut","pachy")
ref_name = "pheno_reindex/sugarcane_disease.phen"
rrecord = data.frame(matrix(nrow=0,ncol=5))
colnames(rrecord) = c("trait","grm","r","index","r2")

for (v in 1:5){
  for (trait in 1:2){
    for (grm in 1:3){
      print(paste0("v=",v))
      print(traits[trait])
      print(grms[grm])
      path="./"
      bv_name = paste0(path,traits[trait],"_exGBLUP_reindex/",traits[trait],"_",grms[grm],"_",v,".bv")
      r = round(get_auc(bv_name,ref_name,trait,v),digit=3)
      rrecord[nrow(rrecord)+1,] = c(traits[trait],grms[grm],r,v,r^2)
  }
  }
}
rrecord$r=as.numeric(rrecord$r)
rrecord$r2=as.numeric(rrecord$r2)
summary(rrecord[which(rrecord$trait == "smut" & rrecord$grm == "a"),])
```



Memorable attention (Imaging attention) 

Ordinal regression for scoring and ranking needs

using pandas bins to get  a series of discrete levels from phenotypes (use for Asynchronous training)

Bing: Yes, both Keras and PyTorch support asynchronous training, where different parts of the model can be trained using different loss functions. In your example, you could define a custom layer that contains two weight matrices and use two separate loss functions (MSE and cross-entropy) to update each weight matrix during training.

Here is an example of how you could implement this in Keras:

```python
from tensorflow.keras import layers

class CustomLayer(layers.Layer):
    def __init__(self, units, **kwargs):
        super().__init__(**kwargs)
        self.units = units

    def build(self, input_shape):
        self.kernel1 = self.add_weight(
            shape=(input_shape[-1], self.units),
            initializer='glorot_uniform',
            name='kernel1'
        )
        self.bias1 = self.add_weight(
            shape=(self.units,),
            initializer='zeros',
            name='bias1'
        )
        self.kernel2 = self.add_weight(
            shape=(input_shape[-1], self.units),
            initializer='glorot_uniform',
            name='kernel2'
        )
        self.bias2 = self.add_weight(
            shape=(self.units,),
            initializer='zeros',
            name='bias2'
        )

    def call(self, inputs):
        output1 = tf.matmul(inputs, self.kernel1) + self.bias1
        output2 = tf.matmul(inputs, self.kernel2) + self.bias2
        return [output1, output2]
```

This custom layer takes as input the number of units and outputs two tensors representing the outputs of the two weight matrices. You can use this layer in your Keras model and define two separate loss functions to update each weight matrix during training.

Here is an example of how you could use this layer in a simple Keras model:

```python
from tensorflow.keras import models

num_features = ... # number of input features
num_units = ... # number of units in the custom layer

inputs = layers.Input(shape=(num_features,))
outputs = CustomLayer(num_units)(inputs)
model = models.Model(inputs=inputs, outputs=outputs)

# compile the model with two separate loss functions for each output
model.compile(loss=['mean_squared_error', 'categorical_crossentropy'], ...)
model.fit(...)
```

In this example, we compile the model with two separate loss functions: MSE for the first output and cross-entropy for the second output. During training, each weight matrix will be updated using its corresponding loss function.

I hope this helps!

Here is an example of how you could implement an ordinal regression output layer using Keras:

```python
from tensorflow.keras import layers

class OrdinalOutputLayer(layers.Layer):
    def __init__(self, num_classes, **kwargs):
        super().__init__(**kwargs)
        self.num_classes = num_classes

    def build(self, input_shape):
        self.kernel = self.add_weight(
            shape=(input_shape[-1], self.num_classes),
            initializer='glorot_uniform',
            name='kernel'
        )
        self.bias = self.add_weight(
            shape=(self.num_classes,),
            initializer='zeros',
            name='bias'
        )

    def call(self, inputs):
        logits = tf.matmul(inputs, self.kernel) + self.bias
        probabilities = tf.math.sigmoid(logits)
        return probabilities
```

This layer takes as input the number of classes (i.e., the number of severity levels) and outputs a tensor of shape `(batch_size, num_classes)` representing the cumulative probabilities for each severity level. You can use this layer as the final layer in your Keras model and train it using an appropriate loss function for ordinal regression.

Here is an example of how you could use this layer in a simple Keras model:

```python
from tensorflow.keras import models

num_features = ... # number of input features
num_classes = 4 # number of severity levels (Lv1-Lv4)

model = models.Sequential([
    layers.Dense(32, activation='relu', input_shape=(num_features,)),
    layers.Dense(16, activation='relu'),
    OrdinalOutputLayer(num_classes)
])

# compile and train the model using an appropriate loss function for ordinal regression
model.compile(...)
model.fit(...)
```

```bash
######using subset, ordinal regression########
run
source ~/.bash_profile

locat=`pwd`
geno="data/subset/disease_subset"
pheno="data/subset/disease_subset.phen"
index="data/subset/disease_subset.index"
anno="data/subset/disease_subset.anno"
target="./Attention_Test"
model="MultiHead Attention LNN"
width=256
depth=2
heads=2
composer="/afm03/Q4/Q4179/ML_composer/"
ls -lh $composer
cp -r $composer $TMPDIR/
python $TMPDIR/ML_composer/GS_composer.py --ped $geno --pheno $pheno --mpheno 1 --index $index --annotation $anno --trait smut --width $width --depth $depth --data-type ordinal --model "MultiHead Attention LNN" -o $target --quiet 1 --round 1 --plot --epoch 10 --embedding 32 --num-heads 2 --batch 64 --lr 0.0001 --residual

```

