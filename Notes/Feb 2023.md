ToDo List

1. AIGS sugarcane revision 
2. Multi head block attention network
3. posterior distribution detection
4. shap investigation 



Bayesian investigate

1. posterior inclusion probability (summary Bayesian)
2. 200511_SBayes_Jackie_Kiewa.pptx (details)

Machine learning parental selection
Khaki S, Khalilzadeh Z, Wang L (2020) Predicting yield performance of parents in plant breeding: A neural collaborative filtering approach. PLOS ONE 15(5): e0233382. https://doi.org/10.1371/journal.pone.0233382



Song, W., Shi, C., Xiao, Z., Duan, Z., Xu, Y., Zhang, M., & Tang, J. (2019, November). Autoint: Automatic feature interaction learning via self-attentive neural networks. In *Proceedings of the 28th ACM International Conference on Information and Knowledge Management* (pp. 1161-1170).

![AutoInt:使用Multi-head Self-Attention进行自动特征学习的CTR模型](https://picx.zhimg.com/v2-4875432d62160ab97a87a17d900dd704_720w.jpg?source=172ae18b)

```bash
composer="/afm03/Q4/Q4179/ML_composer/"
ls -lh $composer
cp -r $composer $TMPDIR/
locat=`pwd`
geno="data/sugarcane_disease"
pheno="data/sugarcane_disease.phen"
index="data/sugarcane_disease.index"
target="./Attention_Test"
model="MultiHead Attention LNN"
width=256
depth=2

python $TMPDIR/ML_composer/GS_composer.py --ped $geno --pheno $pheno --mpheno 1 --index $index --trait smut --width $width --depth $depth --model "MultiHead Attention LNN" -o $target --quiet 1 --plot --epoch 15 --num-heads 2

python $TMPDIR/ML_composer/GS_composer.py --ped $geno --pheno $pheno --mpheno 1 --index $index --trait smut --width $width --depth $depth --model "Attention CNN" -o $target --quiet 1 --plot --epoch 15 --num-heads 2 --lr 0.1 --residual
#On submit script
python $TMPDIR/ML_composer/GS_composer.py --ped $geno --pheno $pheno --mpheno 1 --index $index --trait smut --width $width --depth $depth --model "Attention CNN" -o $target --quiet 0 --plot --epoch 80 --num-heads 2 --lr 0.01 --residual
python $TMPDIR/ML_composer/GS_composer.py --ped $geno --pheno $pheno --mpheno 2 --index $index --trait pachy --width $width --depth $depth --model "Attention CNN" -o $target --quiet 0 --plot --epoch 80 --num-heads 2 --lr 0.01 --residual
```



Self designed SNP global attention:

Raw Q,K,V shape: (batch, seq, 1,embed) or (batch, seq, embed)

Wk = (seq,1,embed) or (seq,embed) = 2608xembed 8 

Wv = (seq,embed) x (embed,second_embed) = (seq,second_embed) = 2606xembed + embedxsecondEmbed 8


$$
Attention\ Score (A) = softmax(Q(W_kK)^T)
$$
Output shape: (batch,Seq,Seq,embed) or (batch, seq, seq) 

Optional (Attention convolution) by (embeding//heads,1), filter: second embedding, step:1 -- get shape (batch, seq, seq,second_embeding) = embedding x second Embed= 64

Kernel shape divided by heads?


$$
Effect\ Score (E) = A(W_V \cdot V)
$$
output shape: (batch,seq,seq,second_embed)

Second convolution: kernel (seq, second_mebed), filter, get output shape: (batch, seq, filter) 2608 x 8

1xfilter convolution,1 filter

Global average pooling

get prediction

overall parameters: 
