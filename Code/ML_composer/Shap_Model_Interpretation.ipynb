{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "142f75eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "import gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f3aadd4e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From I:\\PhD_Notebook\\Code\\ML_composer\\ClassModel.py:31: experimental_run_functions_eagerly (from tensorflow.python.eager.def_function) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.config.run_functions_eagerly` instead of the experimental version.\n"
     ]
    }
   ],
   "source": [
    "from CustomLayers import *\n",
    "from GS_composer import *\n",
    "from Functions import *\n",
    "from ClassModel import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ee65e57c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#user_profile = \"E:/learning resource/\"\n",
    "user_profile = \"D:/\"\n",
    "Model_path = user_profile+\"OneDrive - The University of Queensland/PhD/HPC_Results/Sugarcane_disease/ML/1Head_RealForm32_32LocalB\"\n",
    "#Model_path = \"E:/learning resource/OneDrive - The University of Queensland/PhD/HPC_Results/Sugarcane_disease/ML/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fa653b39",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import backend as K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bc509ea5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:SavedModel saved prior to TF 2.5 detected when loading Keras model. Please ensure that you are saving the model with model.save() or tf.keras.models.save_model(), *NOT* tf.saved_model.save(). To confirm, there should be a file named \"keras_metadata.pb\" in the SavedModel directory.\n",
      "[TensorShape([None, 2608, 32])]\n",
      "[TensorShape([None, 2608, 32]), TensorShape([None, 2608, 32])]\n"
     ]
    }
   ],
   "source": [
    "model_folder = \"pachy_MultiHeadAttentionLNN_1\"\n",
    "#model_folder = \"test_model\"\n",
    "full_path = Model_path + \"/\" + model_folder\n",
    "model = keras.models.load_model(full_path,custom_objects={\"MultiHead_QKV_BlockAttention\": MultiHead_QKV_BlockAttention,\n",
    "                                                         \"SNPBlockLayer\":SNPBlockLayer})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8073c3c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_layer_1 (InputLayer)      [(None, 26086, 1)]   0                                            \n",
      "__________________________________________________________________________________________________\n",
      "zero_padding1d (ZeroPadding1D)  (None, 26086, 1)     0           input_layer_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "locally_connected1d (LocallyCon (None, 2608, 32)     834560      zero_padding1d[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, 2608, 32)     1056        locally_connected1d[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "multi_head_qkv__block_attention ((2608, 32), (2608,  3200        dense[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "add (Add)                       (None, 2608, 32)     0           multi_head_qkv__block_attention[0\n",
      "                                                                 dense[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization (LayerNorma (None, 2608, 32)     64          add[0][0]                        \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 2608, 32)     1056        layer_normalization[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "add_1 (Add)                     (None, 2608, 32)     0           dense_1[0][0]                    \n",
      "                                                                 layer_normalization[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "re_lu (ReLU)                    multiple             0           add_1[0][0]                      \n",
      "                                                                 add_3[0][0]                      \n",
      "                                                                 add_4[0][0]                      \n",
      "                                                                 dense_5[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "multi_head_qkv__block_attention ((None, 2608, 32), ( 3200        re_lu[0][0]                      \n",
      "                                                                 multi_head_qkv__block_attention[0\n",
      "__________________________________________________________________________________________________\n",
      "add_2 (Add)                     (None, 2608, 32)     0           multi_head_qkv__block_attention_1\n",
      "                                                                 re_lu[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_1 (LayerNor (None, 2608, 32)     64          add_2[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 2608, 32)     1056        layer_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "add_3 (Add)                     (None, 2608, 32)     0           dense_2[0][0]                    \n",
      "                                                                 layer_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "flatten (Flatten)               (None, 83456)        0           re_lu[1][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "dropout (Dropout)               (None, 83456)        0           flatten[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, 256)          21364992    dropout[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_4 (Dense)                 (None, 256)          65792       dense_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "add_4 (Add)                     (None, 256)          0           dense_3[0][0]                    \n",
      "                                                                 dense_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_5 (Dense)                 (None, 256)          65792       re_lu[2][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "dense_6 (Dense)                 (None, 1)            257         re_lu[3][0]                      \n",
      "==================================================================================================\n",
      "Total params: 22,341,089\n",
      "Trainable params: 22,341,089\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()\n",
    "#model = keras.Model(inputs=input1, outputs=QV_output)\n",
    "model.compile(optimizer=\"RMSprop\", loss=\"mean_squared_error\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ae907fa",
   "metadata": {},
   "source": [
    "A test variable of SNP array based on the freq of alleles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a8f3b95a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 26086, 1)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#data = np.ones((1, 26086, 1))\n",
    "test_filename = user_profile+\"OneDrive - The University of Queensland/PhD/data/sugarcane_disease/sugarcane_disease.freqV1.ped\"\n",
    "data = pd.read_csv(test_filename,header=None,sep=\"\\t\").iloc[:,6:]\n",
    "data=np.expand_dims(data,axis=-1)\n",
    "print(data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbb01d8f",
   "metadata": {},
   "source": [
    "Model predict via test freq data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5ce96da6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I:\\miniconda\\envs\\tf25\\lib\\site-packages\\tensorflow\\python\\data\\ops\\dataset_ops.py:3704: UserWarning: Even though the `tf.config.experimental_run_functions_eagerly` option is set, this option does not apply to tf.data functions. To force eager execution of tf.data functions, please use `tf.data.experimental.enable.debug_mode()`.\n",
      "  \"Even though the `tf.config.experimental_run_functions_eagerly` \"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[3.8025415]], dtype=float32)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8649526",
   "metadata": {},
   "source": [
    "Extract layer weights and outputs from the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "da1d3688",
   "metadata": {},
   "outputs": [],
   "source": [
    "intermediate_layer_model = tf.keras.Model(inputs=model.input,\n",
    "                                         outputs=model.get_layer('dense_2').output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "ebdb3c1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "intermediate_output = intermediate_layer_model.predict(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "5a1f8792",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2608, 1, 32)\n"
     ]
    }
   ],
   "source": [
    "attention = intermediate_output[0]\n",
    "attention = K.expand_dims(attention,axis=1)\n",
    "print(attention.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "8a36632c",
   "metadata": {},
   "outputs": [],
   "source": [
    "LNN_weight = model.layers[2].get_weights()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "da661fea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2608, 10, 32)"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "LNN_weight[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "72ea7141",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2608, 10, 32)"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "LNN_weight[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "d2547401",
   "metadata": {},
   "outputs": [],
   "source": [
    "SNP_attention = tf.multiply(attention,LNN_weight[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "c601049d",
   "metadata": {},
   "outputs": [],
   "source": [
    "SNP_attention = tf.reshape(SNP_attention,(26080,1,32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "22370707",
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_SNP_attention = SNP_attention\n",
    "#mean_SNP_attention = tf.reduce_mean(SNP_attention,axis=2)\n",
    "mean_SNP_attention.shape\n",
    "#mean_SNP_attention =np.array(tf.reshape(mean_SNP_attention,(26080,))) #np.array(tf.reshape(mean_SNP_attention,(26080*32,)))\n",
    "mean_SNP_attention =np.array(tf.reshape(mean_SNP_attention,(26080,32)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "b1dba58a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "57a9c8ef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.273687e-05"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(mean_SNP_attention)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "4e4bbc21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(26080, 32)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.collections.PathCollection at 0x203d2aaaa08>"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(mean_SNP_attention.shape)\n",
    "#plt.scatter(x=np.tile(np.arange(0,26080),1),y=abs(mean_SNP_attention))\n",
    "plt.scatter(x=np.tile(np.arange(0,26080),32),y=mean_SNP_attention)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "daaab8f8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(834560,)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_SNP_attention.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "6ea635e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def export_attention_score_by_trait(filename,trait,val):\n",
    "    model = keras.models.load_model(filename,custom_objects={\"MultiHead_QKV_BlockAttention\": MultiHead_QKV_BlockAttention,\n",
    "                                                         \"SNPBlockLayer\":SNPBlockLayer})\n",
    "    model.summary()\n",
    "    #model = keras.Model(inputs=input1, outputs=QV_output)\n",
    "    model.compile(optimizer=\"RMSprop\", loss=\"mean_squared_error\")\n",
    "    test_filename = user_profile+\"OneDrive - The University of Queensland/PhD/data/sugarcane_disease/sugarcane_disease.freqV1.ped\"\n",
    "    data = pd.read_csv(test_filename,header=None,sep=\"\\t\").iloc[:,6:]\n",
    "    data=np.expand_dims(data,axis=-1)\n",
    "    print(data.shape)\n",
    "    intermediate_layer_model = tf.keras.Model(inputs=model.input,\n",
    "                                         outputs=model.get_layer('add_3').output)\n",
    "    intermediate_output = intermediate_layer_model.predict(data)\n",
    "    attention = intermediate_output[0]\n",
    "    attention = K.expand_dims(attention,axis=1)\n",
    "    print(attention.shape)\n",
    "    \n",
    "    LNN_weight = model.layers[2].get_weights()\n",
    "    SNP_attention = tf.multiply(attention,LNN_weight[0])\n",
    "    SNP_attention = np.array(tf.reshape(SNP_attention,(32,26080)))\n",
    "    SNP_attention = pd.DataFrame(SNP_attention,columns=list(range(1,SNP_attention.shape[1]+1)))\n",
    "    SNP_attention.shape\n",
    "    SNP_attention.insert(0,'Channel',range(1,33))\n",
    "    SNP_attention.insert(0,'Val',val)\n",
    "    SNP_attention.insert(0,\"Trait\",trait)\n",
    "    \n",
    "    SNP_attention = SNP_attention.values.tolist()\n",
    "    #mean_SNP_attention = [trait,val]+list(np.array(tf.reshape(mean_SNP_attention,(26080,))))\n",
    "    #mean_SNP_attention = pd.DataFrame([trait,val]+list(mean_SNP_attention),columns=[trait,val]+list(range(1,length(mean_SNP_attention))))\n",
    "    model = None\n",
    "    keras.backend.clear_session()\n",
    "    gc.collect()\n",
    "    \n",
    "    return SNP_attention\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "c0647f07",
   "metadata": {},
   "outputs": [],
   "source": [
    "def export_LD_attention_score_by_trait(filename,trait,val):\n",
    "    model = keras.models.load_model(filename,custom_objects={\"MultiHead_QKV_BlockAttention\": MultiHead_QKV_BlockAttention,\n",
    "                                                         \"SNPBlockLayer\":SNPBlockLayer})\n",
    "    model.summary()\n",
    "    #model = keras.Model(inputs=input1, outputs=QV_output)\n",
    "    model.compile(optimizer=\"RMSprop\", loss=\"mean_squared_error\")\n",
    "    test_filename = user_profile+\"OneDrive - The University of Queensland/PhD/data/sugarcane_disease/sugarcane_disease.freqV1.ped\"\n",
    "    data = pd.read_csv(test_filename,header=None,sep=\"\\t\").iloc[:,6:]\n",
    "    data=np.expand_dims(data,axis=-1)\n",
    "    print(data.shape)\n",
    "    intermediate_layer_model = tf.keras.Model(inputs=model.input,\n",
    "                                         outputs=model.get_layer('multi_head_qkv__block_attention').output)\n",
    "    intermediate_output = intermediate_layer_model.predict(data)\n",
    "    attention = intermediate_output[0]\n",
    "    attention = K.expand_dims(attention,axis=1)\n",
    "    print(attention.shape)\n",
    "    \n",
    "    #LNN_weight = model.layers[2].get_weights()\n",
    "    #SNP_attention = tf.multiply(attention,LNN_weight[0])\n",
    "    LD_attention = np.array(tf.reshape(attention,(32,2608)))\n",
    "    LD_attention = pd.DataFrame(LD_attention,columns=list(range(1,LD_attention.shape[1]+1)))\n",
    "\n",
    "    LD_attention.insert(0,'Channel',range(1,33))\n",
    "    LD_attention.insert(0,'Val',val)\n",
    "    LD_attention.insert(0,\"Trait\",trait)\n",
    "    \n",
    "    LD_attention = LD_attention.values.tolist()\n",
    "    #mean_SNP_attention = [trait,val]+list(np.array(tf.reshape(mean_SNP_attention,(26080,))))\n",
    "    #mean_SNP_attention = pd.DataFrame([trait,val]+list(mean_SNP_attention),columns=[trait,val]+list(range(1,length(mean_SNP_attention))))\n",
    "    model = None\n",
    "    keras.backend.clear_session()\n",
    "    gc.collect()\n",
    "    \n",
    "    return LD_attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "1ab7a763",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:SavedModel saved prior to TF 2.5 detected when loading Keras model. Please ensure that you are saving the model with model.save() or tf.keras.models.save_model(), *NOT* tf.saved_model.save(). To confirm, there should be a file named \"keras_metadata.pb\" in the SavedModel directory.\n",
      "[TensorShape([None, 2608, 32])]\n",
      "[TensorShape([None, 2608, 32]), TensorShape([None, 2608, 32])]\n",
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_layer_1 (InputLayer)      [(None, 26086, 1)]   0                                            \n",
      "__________________________________________________________________________________________________\n",
      "zero_padding1d (ZeroPadding1D)  (None, 26086, 1)     0           input_layer_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "locally_connected1d (LocallyCon (None, 2608, 32)     834560      zero_padding1d[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, 2608, 32)     1056        locally_connected1d[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "multi_head_qkv__block_attention ((2608, 32), (2608,  3200        dense[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "add (Add)                       (None, 2608, 32)     0           multi_head_qkv__block_attention[0\n",
      "                                                                 dense[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization (LayerNorma (None, 2608, 32)     64          add[0][0]                        \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 2608, 32)     1056        layer_normalization[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "add_1 (Add)                     (None, 2608, 32)     0           dense_1[0][0]                    \n",
      "                                                                 layer_normalization[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "re_lu (ReLU)                    multiple             0           add_1[0][0]                      \n",
      "                                                                 add_3[0][0]                      \n",
      "                                                                 add_4[0][0]                      \n",
      "                                                                 dense_5[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "multi_head_qkv__block_attention ((None, 2608, 32), ( 3200        re_lu[0][0]                      \n",
      "                                                                 multi_head_qkv__block_attention[0\n",
      "__________________________________________________________________________________________________\n",
      "add_2 (Add)                     (None, 2608, 32)     0           multi_head_qkv__block_attention_1\n",
      "                                                                 re_lu[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_1 (LayerNor (None, 2608, 32)     64          add_2[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 2608, 32)     1056        layer_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "add_3 (Add)                     (None, 2608, 32)     0           dense_2[0][0]                    \n",
      "                                                                 layer_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "flatten (Flatten)               (None, 83456)        0           re_lu[1][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "dropout (Dropout)               (None, 83456)        0           flatten[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, 256)          21364992    dropout[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_4 (Dense)                 (None, 256)          65792       dense_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "add_4 (Add)                     (None, 256)          0           dense_3[0][0]                    \n",
      "                                                                 dense_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_5 (Dense)                 (None, 256)          65792       re_lu[2][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "dense_6 (Dense)                 (None, 1)            257         re_lu[3][0]                      \n",
      "==================================================================================================\n",
      "Total params: 22,341,089\n",
      "Trainable params: 22,341,089\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "(1, 26086, 1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I:\\miniconda\\envs\\tf25\\lib\\site-packages\\tensorflow\\python\\data\\ops\\dataset_ops.py:3704: UserWarning: Even though the `tf.config.experimental_run_functions_eagerly` option is set, this option does not apply to tf.data functions. To force eager execution of tf.data functions, please use `tf.data.experimental.enable.debug_mode()`.\n",
      "  \"Even though the `tf.config.experimental_run_functions_eagerly` \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2608, 1, 32)\n",
      "WARNING:tensorflow:SavedModel saved prior to TF 2.5 detected when loading Keras model. Please ensure that you are saving the model with model.save() or tf.keras.models.save_model(), *NOT* tf.saved_model.save(). To confirm, there should be a file named \"keras_metadata.pb\" in the SavedModel directory.\n",
      "[TensorShape([None, 2608, 32])]\n",
      "[TensorShape([None, 2608, 32]), TensorShape([None, 2608, 32])]\n",
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_layer_1 (InputLayer)      [(None, 26086, 1)]   0                                            \n",
      "__________________________________________________________________________________________________\n",
      "zero_padding1d (ZeroPadding1D)  (None, 26086, 1)     0           input_layer_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "locally_connected1d (LocallyCon (None, 2608, 32)     834560      zero_padding1d[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, 2608, 32)     1056        locally_connected1d[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "multi_head_qkv__block_attention ((2608, 32), (2608,  3200        dense[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "add (Add)                       (None, 2608, 32)     0           multi_head_qkv__block_attention[0\n",
      "                                                                 dense[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization (LayerNorma (None, 2608, 32)     64          add[0][0]                        \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 2608, 32)     1056        layer_normalization[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "add_1 (Add)                     (None, 2608, 32)     0           dense_1[0][0]                    \n",
      "                                                                 layer_normalization[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "re_lu (ReLU)                    multiple             0           add_1[0][0]                      \n",
      "                                                                 add_3[0][0]                      \n",
      "                                                                 add_4[0][0]                      \n",
      "                                                                 dense_5[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "multi_head_qkv__block_attention ((None, 2608, 32), ( 3200        re_lu[0][0]                      \n",
      "                                                                 multi_head_qkv__block_attention[0\n",
      "__________________________________________________________________________________________________\n",
      "add_2 (Add)                     (None, 2608, 32)     0           multi_head_qkv__block_attention_1\n",
      "                                                                 re_lu[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_1 (LayerNor (None, 2608, 32)     64          add_2[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 2608, 32)     1056        layer_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "add_3 (Add)                     (None, 2608, 32)     0           dense_2[0][0]                    \n",
      "                                                                 layer_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "flatten (Flatten)               (None, 83456)        0           re_lu[1][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "dropout (Dropout)               (None, 83456)        0           flatten[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, 256)          21364992    dropout[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_4 (Dense)                 (None, 256)          65792       dense_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "add_4 (Add)                     (None, 256)          0           dense_3[0][0]                    \n",
      "                                                                 dense_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_5 (Dense)                 (None, 256)          65792       re_lu[2][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "dense_6 (Dense)                 (None, 1)            257         re_lu[3][0]                      \n",
      "==================================================================================================\n",
      "Total params: 22,341,089\n",
      "Trainable params: 22,341,089\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "(1, 26086, 1)\n",
      "(2608, 1, 32)\n",
      "WARNING:tensorflow:SavedModel saved prior to TF 2.5 detected when loading Keras model. Please ensure that you are saving the model with model.save() or tf.keras.models.save_model(), *NOT* tf.saved_model.save(). To confirm, there should be a file named \"keras_metadata.pb\" in the SavedModel directory.\n",
      "[TensorShape([None, 2608, 32])]\n",
      "[TensorShape([None, 2608, 32]), TensorShape([None, 2608, 32])]\n",
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_layer_1 (InputLayer)      [(None, 26086, 1)]   0                                            \n",
      "__________________________________________________________________________________________________\n",
      "zero_padding1d (ZeroPadding1D)  (None, 26086, 1)     0           input_layer_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "locally_connected1d (LocallyCon (None, 2608, 32)     834560      zero_padding1d[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, 2608, 32)     1056        locally_connected1d[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "multi_head_qkv__block_attention ((2608, 32), (2608,  3200        dense[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "add (Add)                       (None, 2608, 32)     0           multi_head_qkv__block_attention[0\n",
      "                                                                 dense[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization (LayerNorma (None, 2608, 32)     64          add[0][0]                        \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 2608, 32)     1056        layer_normalization[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "add_1 (Add)                     (None, 2608, 32)     0           dense_1[0][0]                    \n",
      "                                                                 layer_normalization[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "re_lu (ReLU)                    multiple             0           add_1[0][0]                      \n",
      "                                                                 add_3[0][0]                      \n",
      "                                                                 add_4[0][0]                      \n",
      "                                                                 dense_5[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "multi_head_qkv__block_attention ((None, 2608, 32), ( 3200        re_lu[0][0]                      \n",
      "                                                                 multi_head_qkv__block_attention[0\n",
      "__________________________________________________________________________________________________\n",
      "add_2 (Add)                     (None, 2608, 32)     0           multi_head_qkv__block_attention_1\n",
      "                                                                 re_lu[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_1 (LayerNor (None, 2608, 32)     64          add_2[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 2608, 32)     1056        layer_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "add_3 (Add)                     (None, 2608, 32)     0           dense_2[0][0]                    \n",
      "                                                                 layer_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "flatten (Flatten)               (None, 83456)        0           re_lu[1][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "dropout (Dropout)               (None, 83456)        0           flatten[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, 256)          21364992    dropout[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_4 (Dense)                 (None, 256)          65792       dense_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "add_4 (Add)                     (None, 256)          0           dense_3[0][0]                    \n",
      "                                                                 dense_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_5 (Dense)                 (None, 256)          65792       re_lu[2][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "dense_6 (Dense)                 (None, 1)            257         re_lu[3][0]                      \n",
      "==================================================================================================\n",
      "Total params: 22,341,089\n",
      "Trainable params: 22,341,089\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 26086, 1)\n",
      "(2608, 1, 32)\n",
      "WARNING:tensorflow:SavedModel saved prior to TF 2.5 detected when loading Keras model. Please ensure that you are saving the model with model.save() or tf.keras.models.save_model(), *NOT* tf.saved_model.save(). To confirm, there should be a file named \"keras_metadata.pb\" in the SavedModel directory.\n",
      "[TensorShape([None, 2608, 32])]\n",
      "[TensorShape([None, 2608, 32]), TensorShape([None, 2608, 32])]\n",
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_layer_1 (InputLayer)      [(None, 26086, 1)]   0                                            \n",
      "__________________________________________________________________________________________________\n",
      "zero_padding1d (ZeroPadding1D)  (None, 26086, 1)     0           input_layer_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "locally_connected1d (LocallyCon (None, 2608, 32)     834560      zero_padding1d[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, 2608, 32)     1056        locally_connected1d[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "multi_head_qkv__block_attention ((2608, 32), (2608,  3200        dense[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "add (Add)                       (None, 2608, 32)     0           multi_head_qkv__block_attention[0\n",
      "                                                                 dense[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization (LayerNorma (None, 2608, 32)     64          add[0][0]                        \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 2608, 32)     1056        layer_normalization[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "add_1 (Add)                     (None, 2608, 32)     0           dense_1[0][0]                    \n",
      "                                                                 layer_normalization[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "re_lu (ReLU)                    multiple             0           add_1[0][0]                      \n",
      "                                                                 add_3[0][0]                      \n",
      "                                                                 add_4[0][0]                      \n",
      "                                                                 dense_5[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "multi_head_qkv__block_attention ((None, 2608, 32), ( 3200        re_lu[0][0]                      \n",
      "                                                                 multi_head_qkv__block_attention[0\n",
      "__________________________________________________________________________________________________\n",
      "add_2 (Add)                     (None, 2608, 32)     0           multi_head_qkv__block_attention_1\n",
      "                                                                 re_lu[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_1 (LayerNor (None, 2608, 32)     64          add_2[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 2608, 32)     1056        layer_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "add_3 (Add)                     (None, 2608, 32)     0           dense_2[0][0]                    \n",
      "                                                                 layer_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "flatten (Flatten)               (None, 83456)        0           re_lu[1][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "dropout (Dropout)               (None, 83456)        0           flatten[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, 256)          21364992    dropout[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_4 (Dense)                 (None, 256)          65792       dense_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "add_4 (Add)                     (None, 256)          0           dense_3[0][0]                    \n",
      "                                                                 dense_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_5 (Dense)                 (None, 256)          65792       re_lu[2][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "dense_6 (Dense)                 (None, 1)            257         re_lu[3][0]                      \n",
      "==================================================================================================\n",
      "Total params: 22,341,089\n",
      "Trainable params: 22,341,089\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "(1, 26086, 1)\n",
      "(2608, 1, 32)\n",
      "WARNING:tensorflow:SavedModel saved prior to TF 2.5 detected when loading Keras model. Please ensure that you are saving the model with model.save() or tf.keras.models.save_model(), *NOT* tf.saved_model.save(). To confirm, there should be a file named \"keras_metadata.pb\" in the SavedModel directory.\n",
      "[TensorShape([None, 2608, 32])]\n",
      "[TensorShape([None, 2608, 32]), TensorShape([None, 2608, 32])]\n",
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_layer_1 (InputLayer)      [(None, 26086, 1)]   0                                            \n",
      "__________________________________________________________________________________________________\n",
      "zero_padding1d (ZeroPadding1D)  (None, 26086, 1)     0           input_layer_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "locally_connected1d (LocallyCon (None, 2608, 32)     834560      zero_padding1d[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, 2608, 32)     1056        locally_connected1d[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "multi_head_qkv__block_attention ((2608, 32), (2608,  3200        dense[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "add (Add)                       (None, 2608, 32)     0           multi_head_qkv__block_attention[0\n",
      "                                                                 dense[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization (LayerNorma (None, 2608, 32)     64          add[0][0]                        \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 2608, 32)     1056        layer_normalization[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "add_1 (Add)                     (None, 2608, 32)     0           dense_1[0][0]                    \n",
      "                                                                 layer_normalization[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "re_lu (ReLU)                    multiple             0           add_1[0][0]                      \n",
      "                                                                 add_3[0][0]                      \n",
      "                                                                 add_4[0][0]                      \n",
      "                                                                 dense_5[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "multi_head_qkv__block_attention ((None, 2608, 32), ( 3200        re_lu[0][0]                      \n",
      "                                                                 multi_head_qkv__block_attention[0\n",
      "__________________________________________________________________________________________________\n",
      "add_2 (Add)                     (None, 2608, 32)     0           multi_head_qkv__block_attention_1\n",
      "                                                                 re_lu[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_1 (LayerNor (None, 2608, 32)     64          add_2[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 2608, 32)     1056        layer_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "add_3 (Add)                     (None, 2608, 32)     0           dense_2[0][0]                    \n",
      "                                                                 layer_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "flatten (Flatten)               (None, 83456)        0           re_lu[1][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "dropout (Dropout)               (None, 83456)        0           flatten[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, 256)          21364992    dropout[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_4 (Dense)                 (None, 256)          65792       dense_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "add_4 (Add)                     (None, 256)          0           dense_3[0][0]                    \n",
      "                                                                 dense_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_5 (Dense)                 (None, 256)          65792       re_lu[2][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "dense_6 (Dense)                 (None, 1)            257         re_lu[3][0]                      \n",
      "==================================================================================================\n",
      "Total params: 22,341,089\n",
      "Trainable params: 22,341,089\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 26086, 1)\n",
      "(2608, 1, 32)\n",
      "WARNING:tensorflow:SavedModel saved prior to TF 2.5 detected when loading Keras model. Please ensure that you are saving the model with model.save() or tf.keras.models.save_model(), *NOT* tf.saved_model.save(). To confirm, there should be a file named \"keras_metadata.pb\" in the SavedModel directory.\n",
      "[TensorShape([None, 2608, 32])]\n",
      "[TensorShape([None, 2608, 32]), TensorShape([None, 2608, 32])]\n",
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_layer_1 (InputLayer)      [(None, 26086, 1)]   0                                            \n",
      "__________________________________________________________________________________________________\n",
      "zero_padding1d (ZeroPadding1D)  (None, 26086, 1)     0           input_layer_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "locally_connected1d (LocallyCon (None, 2608, 32)     834560      zero_padding1d[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, 2608, 32)     1056        locally_connected1d[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "multi_head_qkv__block_attention ((2608, 32), (2608,  3200        dense[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "add (Add)                       (None, 2608, 32)     0           multi_head_qkv__block_attention[0\n",
      "                                                                 dense[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization (LayerNorma (None, 2608, 32)     64          add[0][0]                        \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 2608, 32)     1056        layer_normalization[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "add_1 (Add)                     (None, 2608, 32)     0           dense_1[0][0]                    \n",
      "                                                                 layer_normalization[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "re_lu (ReLU)                    multiple             0           add_1[0][0]                      \n",
      "                                                                 add_3[0][0]                      \n",
      "                                                                 add_4[0][0]                      \n",
      "                                                                 dense_5[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "multi_head_qkv__block_attention ((None, 2608, 32), ( 3200        re_lu[0][0]                      \n",
      "                                                                 multi_head_qkv__block_attention[0\n",
      "__________________________________________________________________________________________________\n",
      "add_2 (Add)                     (None, 2608, 32)     0           multi_head_qkv__block_attention_1\n",
      "                                                                 re_lu[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_1 (LayerNor (None, 2608, 32)     64          add_2[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 2608, 32)     1056        layer_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "add_3 (Add)                     (None, 2608, 32)     0           dense_2[0][0]                    \n",
      "                                                                 layer_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "flatten (Flatten)               (None, 83456)        0           re_lu[1][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "dropout (Dropout)               (None, 83456)        0           flatten[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, 256)          21364992    dropout[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_4 (Dense)                 (None, 256)          65792       dense_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "add_4 (Add)                     (None, 256)          0           dense_3[0][0]                    \n",
      "                                                                 dense_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_5 (Dense)                 (None, 256)          65792       re_lu[2][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "dense_6 (Dense)                 (None, 1)            257         re_lu[3][0]                      \n",
      "==================================================================================================\n",
      "Total params: 22,341,089\n",
      "Trainable params: 22,341,089\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "(1, 26086, 1)\n",
      "(2608, 1, 32)\n",
      "WARNING:tensorflow:SavedModel saved prior to TF 2.5 detected when loading Keras model. Please ensure that you are saving the model with model.save() or tf.keras.models.save_model(), *NOT* tf.saved_model.save(). To confirm, there should be a file named \"keras_metadata.pb\" in the SavedModel directory.\n",
      "[TensorShape([None, 2608, 32])]\n",
      "[TensorShape([None, 2608, 32]), TensorShape([None, 2608, 32])]\n",
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_layer_1 (InputLayer)      [(None, 26086, 1)]   0                                            \n",
      "__________________________________________________________________________________________________\n",
      "zero_padding1d (ZeroPadding1D)  (None, 26086, 1)     0           input_layer_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "locally_connected1d (LocallyCon (None, 2608, 32)     834560      zero_padding1d[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, 2608, 32)     1056        locally_connected1d[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "multi_head_qkv__block_attention ((2608, 32), (2608,  3200        dense[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "add (Add)                       (None, 2608, 32)     0           multi_head_qkv__block_attention[0\n",
      "                                                                 dense[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization (LayerNorma (None, 2608, 32)     64          add[0][0]                        \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 2608, 32)     1056        layer_normalization[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "add_1 (Add)                     (None, 2608, 32)     0           dense_1[0][0]                    \n",
      "                                                                 layer_normalization[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "re_lu (ReLU)                    multiple             0           add_1[0][0]                      \n",
      "                                                                 add_3[0][0]                      \n",
      "                                                                 add_4[0][0]                      \n",
      "                                                                 dense_5[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "multi_head_qkv__block_attention ((None, 2608, 32), ( 3200        re_lu[0][0]                      \n",
      "                                                                 multi_head_qkv__block_attention[0\n",
      "__________________________________________________________________________________________________\n",
      "add_2 (Add)                     (None, 2608, 32)     0           multi_head_qkv__block_attention_1\n",
      "                                                                 re_lu[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_1 (LayerNor (None, 2608, 32)     64          add_2[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 2608, 32)     1056        layer_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "add_3 (Add)                     (None, 2608, 32)     0           dense_2[0][0]                    \n",
      "                                                                 layer_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "flatten (Flatten)               (None, 83456)        0           re_lu[1][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "dropout (Dropout)               (None, 83456)        0           flatten[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, 256)          21364992    dropout[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_4 (Dense)                 (None, 256)          65792       dense_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "add_4 (Add)                     (None, 256)          0           dense_3[0][0]                    \n",
      "                                                                 dense_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_5 (Dense)                 (None, 256)          65792       re_lu[2][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "dense_6 (Dense)                 (None, 1)            257         re_lu[3][0]                      \n",
      "==================================================================================================\n",
      "Total params: 22,341,089\n",
      "Trainable params: 22,341,089\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 26086, 1)\n",
      "(2608, 1, 32)\n",
      "WARNING:tensorflow:SavedModel saved prior to TF 2.5 detected when loading Keras model. Please ensure that you are saving the model with model.save() or tf.keras.models.save_model(), *NOT* tf.saved_model.save(). To confirm, there should be a file named \"keras_metadata.pb\" in the SavedModel directory.\n",
      "[TensorShape([None, 2608, 32])]\n",
      "[TensorShape([None, 2608, 32]), TensorShape([None, 2608, 32])]\n",
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_layer_1 (InputLayer)      [(None, 26086, 1)]   0                                            \n",
      "__________________________________________________________________________________________________\n",
      "zero_padding1d (ZeroPadding1D)  (None, 26086, 1)     0           input_layer_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "locally_connected1d (LocallyCon (None, 2608, 32)     834560      zero_padding1d[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, 2608, 32)     1056        locally_connected1d[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "multi_head_qkv__block_attention ((2608, 32), (2608,  3200        dense[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "add (Add)                       (None, 2608, 32)     0           multi_head_qkv__block_attention[0\n",
      "                                                                 dense[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization (LayerNorma (None, 2608, 32)     64          add[0][0]                        \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 2608, 32)     1056        layer_normalization[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "add_1 (Add)                     (None, 2608, 32)     0           dense_1[0][0]                    \n",
      "                                                                 layer_normalization[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "re_lu (ReLU)                    multiple             0           add_1[0][0]                      \n",
      "                                                                 add_3[0][0]                      \n",
      "                                                                 add_4[0][0]                      \n",
      "                                                                 dense_5[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "multi_head_qkv__block_attention ((None, 2608, 32), ( 3200        re_lu[0][0]                      \n",
      "                                                                 multi_head_qkv__block_attention[0\n",
      "__________________________________________________________________________________________________\n",
      "add_2 (Add)                     (None, 2608, 32)     0           multi_head_qkv__block_attention_1\n",
      "                                                                 re_lu[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_1 (LayerNor (None, 2608, 32)     64          add_2[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 2608, 32)     1056        layer_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "add_3 (Add)                     (None, 2608, 32)     0           dense_2[0][0]                    \n",
      "                                                                 layer_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "flatten (Flatten)               (None, 83456)        0           re_lu[1][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "dropout (Dropout)               (None, 83456)        0           flatten[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, 256)          21364992    dropout[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_4 (Dense)                 (None, 256)          65792       dense_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "add_4 (Add)                     (None, 256)          0           dense_3[0][0]                    \n",
      "                                                                 dense_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_5 (Dense)                 (None, 256)          65792       re_lu[2][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "dense_6 (Dense)                 (None, 1)            257         re_lu[3][0]                      \n",
      "==================================================================================================\n",
      "Total params: 22,341,089\n",
      "Trainable params: 22,341,089\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "(1, 26086, 1)\n",
      "(2608, 1, 32)\n",
      "WARNING:tensorflow:SavedModel saved prior to TF 2.5 detected when loading Keras model. Please ensure that you are saving the model with model.save() or tf.keras.models.save_model(), *NOT* tf.saved_model.save(). To confirm, there should be a file named \"keras_metadata.pb\" in the SavedModel directory.\n",
      "[TensorShape([None, 2608, 32])]\n",
      "[TensorShape([None, 2608, 32]), TensorShape([None, 2608, 32])]\n",
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_layer_1 (InputLayer)      [(None, 26086, 1)]   0                                            \n",
      "__________________________________________________________________________________________________\n",
      "zero_padding1d (ZeroPadding1D)  (None, 26086, 1)     0           input_layer_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "locally_connected1d (LocallyCon (None, 2608, 32)     834560      zero_padding1d[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, 2608, 32)     1056        locally_connected1d[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "multi_head_qkv__block_attention ((2608, 32), (2608,  3200        dense[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "add (Add)                       (None, 2608, 32)     0           multi_head_qkv__block_attention[0\n",
      "                                                                 dense[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization (LayerNorma (None, 2608, 32)     64          add[0][0]                        \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 2608, 32)     1056        layer_normalization[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "add_1 (Add)                     (None, 2608, 32)     0           dense_1[0][0]                    \n",
      "                                                                 layer_normalization[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "re_lu (ReLU)                    multiple             0           add_1[0][0]                      \n",
      "                                                                 add_3[0][0]                      \n",
      "                                                                 add_4[0][0]                      \n",
      "                                                                 dense_5[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "multi_head_qkv__block_attention ((None, 2608, 32), ( 3200        re_lu[0][0]                      \n",
      "                                                                 multi_head_qkv__block_attention[0\n",
      "__________________________________________________________________________________________________\n",
      "add_2 (Add)                     (None, 2608, 32)     0           multi_head_qkv__block_attention_1\n",
      "                                                                 re_lu[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_1 (LayerNor (None, 2608, 32)     64          add_2[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 2608, 32)     1056        layer_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "add_3 (Add)                     (None, 2608, 32)     0           dense_2[0][0]                    \n",
      "                                                                 layer_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "flatten (Flatten)               (None, 83456)        0           re_lu[1][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "dropout (Dropout)               (None, 83456)        0           flatten[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, 256)          21364992    dropout[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_4 (Dense)                 (None, 256)          65792       dense_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "add_4 (Add)                     (None, 256)          0           dense_3[0][0]                    \n",
      "                                                                 dense_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_5 (Dense)                 (None, 256)          65792       re_lu[2][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "dense_6 (Dense)                 (None, 1)            257         re_lu[3][0]                      \n",
      "==================================================================================================\n",
      "Total params: 22,341,089\n",
      "Trainable params: 22,341,089\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 26086, 1)\n",
      "(2608, 1, 32)\n",
      "WARNING:tensorflow:SavedModel saved prior to TF 2.5 detected when loading Keras model. Please ensure that you are saving the model with model.save() or tf.keras.models.save_model(), *NOT* tf.saved_model.save(). To confirm, there should be a file named \"keras_metadata.pb\" in the SavedModel directory.\n",
      "[TensorShape([None, 2608, 32])]\n",
      "[TensorShape([None, 2608, 32]), TensorShape([None, 2608, 32])]\n",
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_layer_1 (InputLayer)      [(None, 26086, 1)]   0                                            \n",
      "__________________________________________________________________________________________________\n",
      "zero_padding1d (ZeroPadding1D)  (None, 26086, 1)     0           input_layer_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "locally_connected1d (LocallyCon (None, 2608, 32)     834560      zero_padding1d[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, 2608, 32)     1056        locally_connected1d[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "multi_head_qkv__block_attention ((2608, 32), (2608,  3200        dense[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "add (Add)                       (None, 2608, 32)     0           multi_head_qkv__block_attention[0\n",
      "                                                                 dense[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization (LayerNorma (None, 2608, 32)     64          add[0][0]                        \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 2608, 32)     1056        layer_normalization[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "add_1 (Add)                     (None, 2608, 32)     0           dense_1[0][0]                    \n",
      "                                                                 layer_normalization[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "re_lu (ReLU)                    multiple             0           add_1[0][0]                      \n",
      "                                                                 add_3[0][0]                      \n",
      "                                                                 add_4[0][0]                      \n",
      "                                                                 dense_5[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "multi_head_qkv__block_attention ((None, 2608, 32), ( 3200        re_lu[0][0]                      \n",
      "                                                                 multi_head_qkv__block_attention[0\n",
      "__________________________________________________________________________________________________\n",
      "add_2 (Add)                     (None, 2608, 32)     0           multi_head_qkv__block_attention_1\n",
      "                                                                 re_lu[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_1 (LayerNor (None, 2608, 32)     64          add_2[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 2608, 32)     1056        layer_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "add_3 (Add)                     (None, 2608, 32)     0           dense_2[0][0]                    \n",
      "                                                                 layer_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "flatten (Flatten)               (None, 83456)        0           re_lu[1][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "dropout (Dropout)               (None, 83456)        0           flatten[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, 256)          21364992    dropout[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_4 (Dense)                 (None, 256)          65792       dense_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "add_4 (Add)                     (None, 256)          0           dense_3[0][0]                    \n",
      "                                                                 dense_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_5 (Dense)                 (None, 256)          65792       re_lu[2][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "dense_6 (Dense)                 (None, 1)            257         re_lu[3][0]                      \n",
      "==================================================================================================\n",
      "Total params: 22,341,089\n",
      "Trainable params: 22,341,089\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "(1, 26086, 1)\n",
      "(2608, 1, 32)\n"
     ]
    }
   ],
   "source": [
    "Model_path = user_profile+\"OneDrive - The University of Queensland/PhD/HPC_Results/Sugarcane_disease/ML/1Head_RealForm32_32LocalB/\"\n",
    "records = []\n",
    "for trait in [\"smut\",\"pachy\"]:\n",
    "    for val in range(1,6):\n",
    "        filename = Model_path + trait + \"_MultiHeadAttentionLNN_\"+str(val)\n",
    "        record = export_attention_score_by_trait(filename,trait,val)\n",
    "        records.append(record)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "3443483d",
   "metadata": {},
   "outputs": [],
   "source": [
    "recordss = []\n",
    "for i in records:\n",
    "    for j in i:\n",
    "        recordss.append(j)\n",
    "records_df = pd.DataFrame(recordss,columns=[\"Trait\",\"Val\",\"Channel\"]+list(range(1,26081)))\n",
    "output_path = Model_path + \"Attention_block_Normscore.csv\"\n",
    "records_df.to_csv(output_path,sep=\"\\t\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "ab7c2c18",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "26083"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(recordss[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c403783f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "75e6262a",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (1377074899.py, line 3)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"C:\\Users\\70651\\AppData\\Local\\Temp\\ipykernel_19132\\1377074899.py\"\u001b[1;36m, line \u001b[1;32m3\u001b[0m\n\u001b[1;33m    records.to_csv(output_path,sep=\"\\t\",index=False)\u001b[0m\n\u001b[1;37m          ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "records.insert(0,\"Val\", #columns.values[0:2] = [\"Trait\",\"Val\"]\n",
    "output_path = Model_path + \"Attention_score.csv\"\n",
    "records.to_csv(output_path,sep=\"\\t\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "b9ac5fbc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Val</th>\n",
       "      <th>Channel</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>...</th>\n",
       "      <th>2599</th>\n",
       "      <th>2600</th>\n",
       "      <th>2601</th>\n",
       "      <th>2602</th>\n",
       "      <th>2603</th>\n",
       "      <th>2604</th>\n",
       "      <th>2605</th>\n",
       "      <th>2606</th>\n",
       "      <th>2607</th>\n",
       "      <th>2608</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>320.000000</td>\n",
       "      <td>320.000000</td>\n",
       "      <td>320.000000</td>\n",
       "      <td>320.000000</td>\n",
       "      <td>320.000000</td>\n",
       "      <td>320.000000</td>\n",
       "      <td>320.000000</td>\n",
       "      <td>320.000000</td>\n",
       "      <td>320.000000</td>\n",
       "      <td>320.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>320.000000</td>\n",
       "      <td>320.000000</td>\n",
       "      <td>320.000000</td>\n",
       "      <td>320.000000</td>\n",
       "      <td>320.000000</td>\n",
       "      <td>320.000000</td>\n",
       "      <td>320.000000</td>\n",
       "      <td>320.000000</td>\n",
       "      <td>320.000000</td>\n",
       "      <td>320.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>3.000000</td>\n",
       "      <td>16.500000</td>\n",
       "      <td>-0.017779</td>\n",
       "      <td>-0.002690</td>\n",
       "      <td>0.016277</td>\n",
       "      <td>0.020947</td>\n",
       "      <td>0.010649</td>\n",
       "      <td>-0.016615</td>\n",
       "      <td>0.018420</td>\n",
       "      <td>-0.001860</td>\n",
       "      <td>...</td>\n",
       "      <td>0.018420</td>\n",
       "      <td>-0.001860</td>\n",
       "      <td>0.015820</td>\n",
       "      <td>0.002858</td>\n",
       "      <td>-0.010440</td>\n",
       "      <td>0.044026</td>\n",
       "      <td>-0.023237</td>\n",
       "      <td>0.002050</td>\n",
       "      <td>-0.000214</td>\n",
       "      <td>0.000578</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>1.416428</td>\n",
       "      <td>9.247553</td>\n",
       "      <td>0.078586</td>\n",
       "      <td>0.055669</td>\n",
       "      <td>0.056119</td>\n",
       "      <td>0.077730</td>\n",
       "      <td>0.063246</td>\n",
       "      <td>0.058860</td>\n",
       "      <td>0.061881</td>\n",
       "      <td>0.070115</td>\n",
       "      <td>...</td>\n",
       "      <td>0.061881</td>\n",
       "      <td>0.070115</td>\n",
       "      <td>0.062232</td>\n",
       "      <td>0.068766</td>\n",
       "      <td>0.057666</td>\n",
       "      <td>0.058510</td>\n",
       "      <td>0.064442</td>\n",
       "      <td>0.074007</td>\n",
       "      <td>0.082149</td>\n",
       "      <td>0.069883</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.210852</td>\n",
       "      <td>-0.089732</td>\n",
       "      <td>-0.067627</td>\n",
       "      <td>-0.152634</td>\n",
       "      <td>-0.089590</td>\n",
       "      <td>-0.109675</td>\n",
       "      <td>-0.077234</td>\n",
       "      <td>-0.156554</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.077234</td>\n",
       "      <td>-0.156554</td>\n",
       "      <td>-0.124824</td>\n",
       "      <td>-0.091735</td>\n",
       "      <td>-0.110522</td>\n",
       "      <td>-0.069324</td>\n",
       "      <td>-0.138551</td>\n",
       "      <td>-0.108763</td>\n",
       "      <td>-0.124099</td>\n",
       "      <td>-0.143316</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>2.000000</td>\n",
       "      <td>8.750000</td>\n",
       "      <td>-0.048288</td>\n",
       "      <td>-0.032931</td>\n",
       "      <td>-0.026142</td>\n",
       "      <td>-0.006531</td>\n",
       "      <td>-0.032639</td>\n",
       "      <td>-0.061595</td>\n",
       "      <td>-0.030207</td>\n",
       "      <td>-0.037388</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.030207</td>\n",
       "      <td>-0.037388</td>\n",
       "      <td>-0.029761</td>\n",
       "      <td>-0.052967</td>\n",
       "      <td>-0.052900</td>\n",
       "      <td>0.001168</td>\n",
       "      <td>-0.057507</td>\n",
       "      <td>-0.074647</td>\n",
       "      <td>-0.044818</td>\n",
       "      <td>-0.062881</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>3.000000</td>\n",
       "      <td>16.500000</td>\n",
       "      <td>0.004393</td>\n",
       "      <td>-0.012298</td>\n",
       "      <td>0.015574</td>\n",
       "      <td>0.018720</td>\n",
       "      <td>0.006472</td>\n",
       "      <td>-0.014921</td>\n",
       "      <td>0.015464</td>\n",
       "      <td>0.007679</td>\n",
       "      <td>...</td>\n",
       "      <td>0.015464</td>\n",
       "      <td>0.007679</td>\n",
       "      <td>0.026777</td>\n",
       "      <td>-0.001210</td>\n",
       "      <td>-0.019092</td>\n",
       "      <td>0.061060</td>\n",
       "      <td>-0.019885</td>\n",
       "      <td>0.010346</td>\n",
       "      <td>-0.003508</td>\n",
       "      <td>0.001606</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>4.000000</td>\n",
       "      <td>24.250000</td>\n",
       "      <td>0.029316</td>\n",
       "      <td>0.015722</td>\n",
       "      <td>0.041073</td>\n",
       "      <td>0.060808</td>\n",
       "      <td>0.043454</td>\n",
       "      <td>0.016009</td>\n",
       "      <td>0.056848</td>\n",
       "      <td>0.040485</td>\n",
       "      <td>...</td>\n",
       "      <td>0.056848</td>\n",
       "      <td>0.040485</td>\n",
       "      <td>0.063301</td>\n",
       "      <td>0.026704</td>\n",
       "      <td>0.026599</td>\n",
       "      <td>0.084302</td>\n",
       "      <td>0.011146</td>\n",
       "      <td>0.070179</td>\n",
       "      <td>0.036710</td>\n",
       "      <td>0.052452</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>5.000000</td>\n",
       "      <td>32.000000</td>\n",
       "      <td>0.091809</td>\n",
       "      <td>0.148235</td>\n",
       "      <td>0.156276</td>\n",
       "      <td>0.160355</td>\n",
       "      <td>0.140824</td>\n",
       "      <td>0.103139</td>\n",
       "      <td>0.132505</td>\n",
       "      <td>0.111163</td>\n",
       "      <td>...</td>\n",
       "      <td>0.132505</td>\n",
       "      <td>0.111163</td>\n",
       "      <td>0.110843</td>\n",
       "      <td>0.161421</td>\n",
       "      <td>0.111619</td>\n",
       "      <td>0.148535</td>\n",
       "      <td>0.119238</td>\n",
       "      <td>0.116224</td>\n",
       "      <td>0.243824</td>\n",
       "      <td>0.141038</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows  2610 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              Val     Channel           1           2           3           4  \\\n",
       "count  320.000000  320.000000  320.000000  320.000000  320.000000  320.000000   \n",
       "mean     3.000000   16.500000   -0.017779   -0.002690    0.016277    0.020947   \n",
       "std      1.416428    9.247553    0.078586    0.055669    0.056119    0.077730   \n",
       "min      1.000000    1.000000   -0.210852   -0.089732   -0.067627   -0.152634   \n",
       "25%      2.000000    8.750000   -0.048288   -0.032931   -0.026142   -0.006531   \n",
       "50%      3.000000   16.500000    0.004393   -0.012298    0.015574    0.018720   \n",
       "75%      4.000000   24.250000    0.029316    0.015722    0.041073    0.060808   \n",
       "max      5.000000   32.000000    0.091809    0.148235    0.156276    0.160355   \n",
       "\n",
       "                5           6           7           8  ...        2599  \\\n",
       "count  320.000000  320.000000  320.000000  320.000000  ...  320.000000   \n",
       "mean     0.010649   -0.016615    0.018420   -0.001860  ...    0.018420   \n",
       "std      0.063246    0.058860    0.061881    0.070115  ...    0.061881   \n",
       "min     -0.089590   -0.109675   -0.077234   -0.156554  ...   -0.077234   \n",
       "25%     -0.032639   -0.061595   -0.030207   -0.037388  ...   -0.030207   \n",
       "50%      0.006472   -0.014921    0.015464    0.007679  ...    0.015464   \n",
       "75%      0.043454    0.016009    0.056848    0.040485  ...    0.056848   \n",
       "max      0.140824    0.103139    0.132505    0.111163  ...    0.132505   \n",
       "\n",
       "             2600        2601        2602        2603        2604        2605  \\\n",
       "count  320.000000  320.000000  320.000000  320.000000  320.000000  320.000000   \n",
       "mean    -0.001860    0.015820    0.002858   -0.010440    0.044026   -0.023237   \n",
       "std      0.070115    0.062232    0.068766    0.057666    0.058510    0.064442   \n",
       "min     -0.156554   -0.124824   -0.091735   -0.110522   -0.069324   -0.138551   \n",
       "25%     -0.037388   -0.029761   -0.052967   -0.052900    0.001168   -0.057507   \n",
       "50%      0.007679    0.026777   -0.001210   -0.019092    0.061060   -0.019885   \n",
       "75%      0.040485    0.063301    0.026704    0.026599    0.084302    0.011146   \n",
       "max      0.111163    0.110843    0.161421    0.111619    0.148535    0.119238   \n",
       "\n",
       "             2606        2607        2608  \n",
       "count  320.000000  320.000000  320.000000  \n",
       "mean     0.002050   -0.000214    0.000578  \n",
       "std      0.074007    0.082149    0.069883  \n",
       "min     -0.108763   -0.124099   -0.143316  \n",
       "25%     -0.074647   -0.044818   -0.062881  \n",
       "50%      0.010346   -0.003508    0.001606  \n",
       "75%      0.070179    0.036710    0.052452  \n",
       "max      0.116224    0.243824    0.141038  \n",
       "\n",
       "[8 rows x 2610 columns]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "records_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "19a20450",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2608, 1, 32)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>2598</th>\n",
       "      <th>2599</th>\n",
       "      <th>2600</th>\n",
       "      <th>2601</th>\n",
       "      <th>2602</th>\n",
       "      <th>2603</th>\n",
       "      <th>2604</th>\n",
       "      <th>2605</th>\n",
       "      <th>2606</th>\n",
       "      <th>2607</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>32.000000</td>\n",
       "      <td>32.000000</td>\n",
       "      <td>32.000000</td>\n",
       "      <td>32.000000</td>\n",
       "      <td>32.000000</td>\n",
       "      <td>32.000000</td>\n",
       "      <td>32.000000</td>\n",
       "      <td>32.000000</td>\n",
       "      <td>32.000000</td>\n",
       "      <td>32.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>32.000000</td>\n",
       "      <td>32.000000</td>\n",
       "      <td>32.000000</td>\n",
       "      <td>32.000000</td>\n",
       "      <td>32.000000</td>\n",
       "      <td>32.000000</td>\n",
       "      <td>32.000000</td>\n",
       "      <td>32.000000</td>\n",
       "      <td>32.000000</td>\n",
       "      <td>32.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.025982</td>\n",
       "      <td>-0.039928</td>\n",
       "      <td>-0.040407</td>\n",
       "      <td>0.034194</td>\n",
       "      <td>-0.028745</td>\n",
       "      <td>0.087532</td>\n",
       "      <td>-0.052552</td>\n",
       "      <td>0.028997</td>\n",
       "      <td>-0.003427</td>\n",
       "      <td>0.022198</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.052552</td>\n",
       "      <td>0.028997</td>\n",
       "      <td>-0.003427</td>\n",
       "      <td>0.022198</td>\n",
       "      <td>-0.040022</td>\n",
       "      <td>0.029404</td>\n",
       "      <td>0.095634</td>\n",
       "      <td>0.049117</td>\n",
       "      <td>-0.036483</td>\n",
       "      <td>0.100009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.002597</td>\n",
       "      <td>0.042523</td>\n",
       "      <td>0.015336</td>\n",
       "      <td>0.029727</td>\n",
       "      <td>0.001629</td>\n",
       "      <td>0.016146</td>\n",
       "      <td>0.024049</td>\n",
       "      <td>0.012860</td>\n",
       "      <td>0.035353</td>\n",
       "      <td>0.003094</td>\n",
       "      <td>...</td>\n",
       "      <td>0.024049</td>\n",
       "      <td>0.012860</td>\n",
       "      <td>0.035353</td>\n",
       "      <td>0.003094</td>\n",
       "      <td>0.013306</td>\n",
       "      <td>0.053255</td>\n",
       "      <td>0.023715</td>\n",
       "      <td>0.039082</td>\n",
       "      <td>0.016013</td>\n",
       "      <td>0.041146</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.023426</td>\n",
       "      <td>-0.081781</td>\n",
       "      <td>-0.055501</td>\n",
       "      <td>0.004935</td>\n",
       "      <td>-0.030349</td>\n",
       "      <td>0.071640</td>\n",
       "      <td>-0.076222</td>\n",
       "      <td>0.016339</td>\n",
       "      <td>-0.038224</td>\n",
       "      <td>0.019152</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.076222</td>\n",
       "      <td>0.016339</td>\n",
       "      <td>-0.038224</td>\n",
       "      <td>0.019152</td>\n",
       "      <td>-0.053118</td>\n",
       "      <td>-0.023012</td>\n",
       "      <td>0.072292</td>\n",
       "      <td>0.010651</td>\n",
       "      <td>-0.052244</td>\n",
       "      <td>0.059512</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.023426</td>\n",
       "      <td>-0.081781</td>\n",
       "      <td>-0.055501</td>\n",
       "      <td>0.004935</td>\n",
       "      <td>-0.030349</td>\n",
       "      <td>0.071640</td>\n",
       "      <td>-0.076222</td>\n",
       "      <td>0.016339</td>\n",
       "      <td>-0.038224</td>\n",
       "      <td>0.019152</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.076222</td>\n",
       "      <td>0.016339</td>\n",
       "      <td>-0.038224</td>\n",
       "      <td>0.019152</td>\n",
       "      <td>-0.053118</td>\n",
       "      <td>-0.023012</td>\n",
       "      <td>0.072292</td>\n",
       "      <td>0.010651</td>\n",
       "      <td>-0.052244</td>\n",
       "      <td>0.059512</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.025982</td>\n",
       "      <td>-0.039928</td>\n",
       "      <td>-0.040407</td>\n",
       "      <td>0.034194</td>\n",
       "      <td>-0.028745</td>\n",
       "      <td>0.087532</td>\n",
       "      <td>-0.052552</td>\n",
       "      <td>0.028997</td>\n",
       "      <td>-0.003427</td>\n",
       "      <td>0.022198</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.052552</td>\n",
       "      <td>0.028997</td>\n",
       "      <td>-0.003427</td>\n",
       "      <td>0.022198</td>\n",
       "      <td>-0.040022</td>\n",
       "      <td>0.029404</td>\n",
       "      <td>0.095634</td>\n",
       "      <td>0.049117</td>\n",
       "      <td>-0.036483</td>\n",
       "      <td>0.100009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.028538</td>\n",
       "      <td>0.001925</td>\n",
       "      <td>-0.025312</td>\n",
       "      <td>0.063452</td>\n",
       "      <td>-0.027142</td>\n",
       "      <td>0.103424</td>\n",
       "      <td>-0.028882</td>\n",
       "      <td>0.041654</td>\n",
       "      <td>0.031370</td>\n",
       "      <td>0.025243</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.028882</td>\n",
       "      <td>0.041654</td>\n",
       "      <td>0.031370</td>\n",
       "      <td>0.025243</td>\n",
       "      <td>-0.026926</td>\n",
       "      <td>0.081821</td>\n",
       "      <td>0.118975</td>\n",
       "      <td>0.087583</td>\n",
       "      <td>-0.020723</td>\n",
       "      <td>0.140507</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>0.028538</td>\n",
       "      <td>0.001925</td>\n",
       "      <td>-0.025312</td>\n",
       "      <td>0.063452</td>\n",
       "      <td>-0.027142</td>\n",
       "      <td>0.103424</td>\n",
       "      <td>-0.028882</td>\n",
       "      <td>0.041654</td>\n",
       "      <td>0.031370</td>\n",
       "      <td>0.025243</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.028882</td>\n",
       "      <td>0.041654</td>\n",
       "      <td>0.031370</td>\n",
       "      <td>0.025243</td>\n",
       "      <td>-0.026926</td>\n",
       "      <td>0.081821</td>\n",
       "      <td>0.118975</td>\n",
       "      <td>0.087583</td>\n",
       "      <td>-0.020723</td>\n",
       "      <td>0.140507</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows  2608 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            0          1          2          3          4          5     \\\n",
       "count  32.000000  32.000000  32.000000  32.000000  32.000000  32.000000   \n",
       "mean    0.025982  -0.039928  -0.040407   0.034194  -0.028745   0.087532   \n",
       "std     0.002597   0.042523   0.015336   0.029727   0.001629   0.016146   \n",
       "min     0.023426  -0.081781  -0.055501   0.004935  -0.030349   0.071640   \n",
       "25%     0.023426  -0.081781  -0.055501   0.004935  -0.030349   0.071640   \n",
       "50%     0.025982  -0.039928  -0.040407   0.034194  -0.028745   0.087532   \n",
       "75%     0.028538   0.001925  -0.025312   0.063452  -0.027142   0.103424   \n",
       "max     0.028538   0.001925  -0.025312   0.063452  -0.027142   0.103424   \n",
       "\n",
       "            6          7          8          9     ...       2598       2599  \\\n",
       "count  32.000000  32.000000  32.000000  32.000000  ...  32.000000  32.000000   \n",
       "mean   -0.052552   0.028997  -0.003427   0.022198  ...  -0.052552   0.028997   \n",
       "std     0.024049   0.012860   0.035353   0.003094  ...   0.024049   0.012860   \n",
       "min    -0.076222   0.016339  -0.038224   0.019152  ...  -0.076222   0.016339   \n",
       "25%    -0.076222   0.016339  -0.038224   0.019152  ...  -0.076222   0.016339   \n",
       "50%    -0.052552   0.028997  -0.003427   0.022198  ...  -0.052552   0.028997   \n",
       "75%    -0.028882   0.041654   0.031370   0.025243  ...  -0.028882   0.041654   \n",
       "max    -0.028882   0.041654   0.031370   0.025243  ...  -0.028882   0.041654   \n",
       "\n",
       "            2600       2601       2602       2603       2604       2605  \\\n",
       "count  32.000000  32.000000  32.000000  32.000000  32.000000  32.000000   \n",
       "mean   -0.003427   0.022198  -0.040022   0.029404   0.095634   0.049117   \n",
       "std     0.035353   0.003094   0.013306   0.053255   0.023715   0.039082   \n",
       "min    -0.038224   0.019152  -0.053118  -0.023012   0.072292   0.010651   \n",
       "25%    -0.038224   0.019152  -0.053118  -0.023012   0.072292   0.010651   \n",
       "50%    -0.003427   0.022198  -0.040022   0.029404   0.095634   0.049117   \n",
       "75%     0.031370   0.025243  -0.026926   0.081821   0.118975   0.087583   \n",
       "max     0.031370   0.025243  -0.026926   0.081821   0.118975   0.087583   \n",
       "\n",
       "            2606       2607  \n",
       "count  32.000000  32.000000  \n",
       "mean   -0.036483   0.100009  \n",
       "std     0.016013   0.041146  \n",
       "min    -0.052244   0.059512  \n",
       "25%    -0.052244   0.059512  \n",
       "50%    -0.036483   0.100009  \n",
       "75%    -0.020723   0.140507  \n",
       "max    -0.020723   0.140507  \n",
       "\n",
       "[8 rows x 2608 columns]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "intermediate_layer_model = tf.keras.Model(inputs=model.input,\n",
    "                                         outputs=model.get_layer('multi_head_qkv__block_attention').output)\n",
    "intermediate_output = intermediate_layer_model.predict(data)\n",
    "attention = intermediate_output[0]\n",
    "attention = K.expand_dims(attention,axis=1)\n",
    "print(attention.shape)\n",
    "pd.DataFrame(np.array(tf.reshape(attention,(32,2608)))).describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "981bf245",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "831217"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keras.backend.clear_session()\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20490f35",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
